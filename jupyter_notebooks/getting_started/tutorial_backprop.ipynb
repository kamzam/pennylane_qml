{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# This cell is added by sphinx-gallery\n# It can be customized to whatever you like\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quantum gradients with backpropagation\n======================================\n\n::: {.meta}\n:property=\\\"og:description\\\": Using backpropagation can speed up\ntraining of quantum circuits compared to the parameter-shift rule---if\nyou are using a simulator.\n\n:property=\\\"og:image\\\":\n<https://pennylane.ai/qml/_images/sphx_glr_tutorial_backprop_002.png>\n:::\n\n::: {.related}\ntutorial\\_quantum\\_natural\\_gradient Quantum natural gradient\n:::\n\n*Author: Josh Izaac --- Posted: 11 August 2020. Last updated: 31 January\n2021.*\n\nIn PennyLane, any quantum device, whether a hardware device or a\nsimulator, can be trained using the\n`parameter-shift rule </glossary/parameter_shift>`{.interpreted-text\nrole=\"doc\"} to compute quantum gradients. Indeed, the parameter-shift\nrule is ideally suited to hardware devices, as it does not require any\nknowledge about the internal workings of the device; it is sufficient to\ntreat the device as a \\'black box\\', and to query it with different\ninput values in order to determine the gradient.\n\nWhen working with simulators, however, we *do* have access to the\ninternal (classical) computations being performed. This allows us to\ntake advantage of other methods of computing the gradient, such as\nbackpropagation, which may be advantageous in certain regimes. In this\ntutorial, we will compare and contrast the parameter-shift rule against\nbackpropagation, using the PennyLane\n`default.qubit <pennylane.devices.default_qubit>`{.interpreted-text\nrole=\"class\"} device.\n\nThe parameter-shift rule\n------------------------\n\nThe parameter-shift rule states that, given a variational quantum\ncircuit $U(\\boldsymbol\n\\theta)$ composed of parametrized Pauli rotations, and some measured\nobservable $\\hat{B}$, the derivative of the expectation value\n\n$$\\langle \\hat{B} \\rangle (\\boldsymbol\\theta) =\n\\langle 0 \\vert U(\\boldsymbol\\theta)^\\dagger \\hat{B} U(\\boldsymbol\\theta) \\vert 0\\rangle$$\n\nwith respect to the input circuit parameters $\\boldsymbol{\\theta}$ is\ngiven by\n\n$$\\nabla_{\\theta_i}\\langle \\hat{B} \\rangle(\\boldsymbol\\theta)\n   =  \\frac{1}{2}\n         \\left[\n             \\langle \\hat{B} \\rangle\\left(\\boldsymbol\\theta + \\frac{\\pi}{2}\\hat{\\mathbf{e}}_i\\right)\n           - \\langle \\hat{B} \\rangle\\left(\\boldsymbol\\theta - \\frac{\\pi}{2}\\hat{\\mathbf{e}}_i\\right)\n         \\right].$$\n\nThus, the gradient of the expectation value can be calculated by\nevaluating the same variational quantum circuit, but with shifted\nparameter values (hence the name, parameter-shift rule!).\n\nLet\\'s have a go implementing the parameter-shift rule manually in\nPennyLane.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pennylane as qml\nfrom pennylane import numpy as np\nfrom matplotlib import pyplot as plt\n\n# set the random seed\nnp.random.seed(42)\n\n# create a device to execute the circuit on\ndev = qml.device(\"default.qubit\", wires=3)\n\n@qml.qnode(dev, diff_method=\"parameter-shift\", interface=\"autograd\")\ndef circuit(params):\n    qml.RX(params[0], wires=0)\n    qml.RY(params[1], wires=1)\n    qml.RZ(params[2], wires=2)\n\n    qml.broadcast(qml.CNOT, wires=[0, 1, 2], pattern=\"ring\")\n\n    qml.RX(params[3], wires=0)\n    qml.RY(params[4], wires=1)\n    qml.RZ(params[5], wires=2)\n\n    qml.broadcast(qml.CNOT, wires=[0, 1, 2], pattern=\"ring\")\n    return qml.expval(qml.PauliY(0) @ qml.PauliZ(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\\'s test the variational circuit evaluation with some parameter\ninput:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# initial parameters\nparams = np.random.random([6], requires_grad=True)\n\nprint(\"Parameters:\", params)\nprint(\"Expectation value:\", circuit(params))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also draw the executed quantum circuit:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = qml.draw_mpl(circuit, decimals=2)(params)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have defined our variational circuit QNode, we can construct\na function that computes the gradient of the $i\\text{th}$ parameter\nusing the parameter-shift rule.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def parameter_shift_term(qnode, params, i):\n    shifted = params.copy()\n    shifted[i] += np.pi/2\n    forward = qnode(shifted)  # forward evaluation\n\n    shifted[i] -= np.pi\n    backward = qnode(shifted) # backward evaluation\n\n    return 0.5 * (forward - backward)\n\n# gradient with respect to the first parameter\nprint(parameter_shift_term(circuit, params, 0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to compute the gradient with respect to *all* parameters, we\nneed to loop over the index `i`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def parameter_shift(qnode, params):\n    gradients = np.zeros([len(params)])\n\n    for i in range(len(params)):\n        gradients[i] = parameter_shift_term(qnode, params, i)\n\n    return gradients\n\nprint(parameter_shift(circuit, params))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can compare this to PennyLane\\'s *built-in* quantum gradient support\nby using the `qml.grad <pennylane.grad>`{.interpreted-text role=\"func\"}\nfunction, which allows us to compute gradients of hybrid\nquantum-classical cost functions. Remember, when we defined the QNode,\nwe specified that we wanted it to be differentiable using the\nparameter-shift method (`diff_method=\"parameter-shift\"`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "grad_function = qml.grad(circuit)\nprint(grad_function(params)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Alternatively, we can directly compute quantum gradients of QNodes using\nPennyLane\\'s built in\n`qml.gradients <pennylane.gradients>`{.interpreted-text role=\"mod\"}\nmodule:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(qml.gradients.param_shift(circuit)(params))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you count the number of quantum evaluations, you will notice that we\nhad to evaluate the circuit `2*len(params)` number of times in order to\ncompute the quantum gradient with respect to all parameters. While\nreasonably fast for a small number of parameters, as the number of\nparameters in our quantum circuit grows, so does both\n\n1.  the circuit depth (and thus the time taken to evaluate each\n    expectation value or \\'forward\\' pass), and\n2.  the number of parameter-shift evaluations required.\n\nBoth of these factors increase the time taken to compute the gradient\nwith respect to all parameters.\n\nBenchmarking\n============\n\nLet\\'s consider an example with a significantly larger number of\nparameters. We\\'ll make use of the\n`~pennylane.StronglyEntanglingLayers`{.interpreted-text role=\"class\"}\ntemplate to make a more complicated QNode.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dev = qml.device(\"default.qubit\", wires=4)\n\n@qml.qnode(dev, diff_method=\"parameter-shift\", interface=\"autograd\")\ndef circuit(params):\n    qml.StronglyEntanglingLayers(params, wires=[0, 1, 2, 3])\n    return qml.expval(qml.PauliZ(0) @ qml.PauliZ(1) @ qml.PauliZ(2) @ qml.PauliZ(3))\n\n# initialize circuit parameters\nparam_shape = qml.StronglyEntanglingLayers.shape(n_wires=4, n_layers=15)\nparams = np.random.normal(scale=0.1, size=param_shape, requires_grad=True)\nprint(params.size)\nprint(circuit(params))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This circuit has 180 parameters. Let\\'s see how long it takes to perform\na forward pass of the circuit.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import timeit\n\nreps = 3\nnum = 10\ntimes = timeit.repeat(\"circuit(params)\", globals=globals(), number=num, repeat=reps)\nforward_time = min(times) / num\n\nprint(f\"Forward pass (best of {reps}): {forward_time} sec per loop\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now estimate the time taken to compute the full gradient vector,\nand see how this compares.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# create the gradient function\ngrad_fn = qml.grad(circuit)\n\ntimes = timeit.repeat(\"grad_fn(params)\", globals=globals(), number=num, repeat=reps)\nbackward_time = min(times) / num\n\nprint(f\"Gradient computation (best of {reps}): {backward_time} sec per loop\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Based on the parameter-shift rule, we expect that the amount of time to\ncompute the quantum gradients should be approximately $2p\\Delta t_{f}$\nwhere $p$ is the number of parameters and $\\Delta t_{f}$ if the time\ntaken for the forward pass. Let\\'s verify this:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(2 * forward_time * params.size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Backpropagation\n===============\n\nAn alternative to the parameter-shift rule for computing gradients is\n[reverse-mode\nautodifferentiation](https://en.wikipedia.org/wiki/Reverse_accumulation).\nUnlike the parameter-shift method, which requires $2p$ circuit\nevaluations for $p$ parameters, reverse-mode requires only a *single*\nforward pass of the differentiable function to compute the gradient of\nall variables, at the expense of increased memory usage. During the\nforward pass, the results of all intermediate subexpressions are stored;\nthe computation is then traversed *in reverse*, with the gradient\ncomputed by repeatedly applying the chain rule. In most classical\nmachine learning settings (where we are training scalar loss functions\nconsisting of a large number of parameters), reverse-mode\nautodifferentiation is the preferred method of\nautodifferentiation\\-\\--the reduction in computational time enables\nlarger and more complex models to be successfully trained. The\nbackpropagation algorithm is a particular special-case of reverse-mode\nautodifferentiation, which has helped lead to the machine learning\nexplosion we see today.\n\nIn quantum machine learning, however, the inability to store and utilize\nthe results of *intermediate* quantum operations on hardware remains a\nbarrier to using backprop; while reverse-mode autodifferentiation works\nfine for small quantum simulations, only the parameter-shift rule can be\nused to compute gradients on quantum hardware directly. Nevertheless,\nwhen training quantum models via classical simulation, it\\'s useful to\nexplore the regimes where reverse-mode differentiation may be a better\nchoice than the parameter-shift rule.\n\nBenchmarking\n------------\n\nWhen creating a QNode,\n`PennyLane supports various methods of differentiation\n<code/api/pennylane.qnode>`{.interpreted-text role=\"doc\"}, including\n`\"parameter-shift\"` (which we used previously), `\"finite-diff\"`,\n`\"reversible\"`, and `\"backprop\"`. While `\"parameter-shift\"` works with\nall devices (simulator or hardware), `\"backprop\"` will only work for\nspecific simulator devices that are designed to support backpropagation.\n\nOne such device is\n`default.qubit <pennylane.devices.DefaultQubit>`{.interpreted-text\nrole=\"class\"}. It has backends written using TensorFlow, JAX, and\nAutograd, so when used with the TensorFlow, JAX, and Autograd interfaces\nrespectively, supports backpropagation. In this demo, we will use the\ndefault Autograd interface.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dev = qml.device(\"default.qubit\", wires=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When defining the QNode, we specify `diff_method=\"backprop\"` to ensure\nthat we are using backpropagation mode. Note that this is the *default\ndifferentiation mode* for the `default.qubit` device.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@qml.qnode(dev, diff_method=\"backprop\", interface=\"autograd\")\ndef circuit(params):\n    qml.StronglyEntanglingLayers(params, wires=[0, 1, 2, 3])\n    return qml.expval(qml.PauliZ(0) @ qml.PauliZ(1) @ qml.PauliZ(2) @ qml.PauliZ(3))\n\n# initialize circuit parameters\nparam_shape = qml.StronglyEntanglingLayers.shape(n_wires=4, n_layers=15)\nparams = np.random.normal(scale=0.1, size=param_shape, requires_grad=True)\nprint(circuit(params))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\\'s see how long it takes to perform a forward pass of the circuit.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import timeit\n\nreps = 3\nnum = 10\ntimes = timeit.repeat(\"circuit(params)\", globals=globals(), number=num, repeat=reps)\nforward_time = min(times) / num\nprint(f\"Forward pass (best of {reps}): {forward_time} sec per loop\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comparing this to the forward pass from `default.qubit`, we note that\nthere is some potential overhead from using backpropagation. We can now\nestimate the time required to perform a gradient computation via\nbackpropagation:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "times = timeit.repeat(\"qml.grad(circuit)(params)\", globals=globals(), number=num, repeat=reps)\nbackward_time = min(times) / num\nprint(f\"Backward pass (best of {reps}): {backward_time} sec per loop\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unlike with the parameter-shift rule, the time taken to perform the\nbackwards pass appears of the order of a single forward pass! This can\nsignificantly speed up training of simulated circuits with many\nparameters.\n\nTime comparison\n===============\n\nLet\\'s compare the two differentiation approaches as the number of\ntrainable parameters in the variational circuit increases, by timing\nboth the forward pass and the gradient computation as the number of\nlayers is allowed to increase.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dev = qml.device(\"default.qubit\", wires=4)\n\ndef circuit(params):\n    qml.StronglyEntanglingLayers(params, wires=[0, 1, 2, 3])\n    return qml.expval(qml.PauliZ(0) @ qml.PauliZ(1) @ qml.PauliZ(2) @ qml.PauliZ(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We\\'ll continue to use the same ansatz as before, but to reduce the time\ntaken to collect the data, we\\'ll reduce the number and repetitions of\ntimings per data point. Below, we loop over a variational circuit depth\nranging from 0 (no gates/ trainable parameters) to 20. Each layer will\ncontain $3N$ parameters, where $N$ is the number of wires (in this case,\nwe have $N=4$).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "reps = 2\nnum = 3\n\nforward_shift = []\ngradient_shift = []\nforward_backprop = []\ngradient_backprop = []\n\nfor depth in range(0, 21):\n    param_shape = qml.StronglyEntanglingLayers.shape(n_wires=4, n_layers=depth)\n    params = np.random.normal(scale=0.1, size=param_shape, requires_grad=True)\n    num_params = params.size\n\n    # forward pass timing\n    # ===================\n\n    qnode_shift = qml.QNode(circuit, dev, diff_method=\"parameter-shift\", interface=\"autograd\")\n    qnode_backprop = qml.QNode(circuit, dev, diff_method=\"backprop\", interface=\"autograd\")\n\n    # parameter-shift\n    t = timeit.repeat(\"qnode_shift(params)\", globals=globals(), number=num, repeat=reps)\n    forward_shift.append([num_params, min(t) / num])\n\n    # backprop\n    t = timeit.repeat(\"qnode_backprop(params)\", globals=globals(), number=num, repeat=reps)\n    forward_backprop.append([num_params, min(t) / num])\n\n    if num_params == 0:\n        continue\n\n    # Gradient timing\n    # ===============\n\n    qnode_shift = qml.QNode(circuit, dev, diff_method=\"parameter-shift\", interface=\"autograd\")\n    qnode_backprop = qml.QNode(circuit, dev, diff_method=\"backprop\", interface=\"autograd\")\n\n    # parameter-shift\n    t = timeit.repeat(\"qml.grad(qnode_shift)(params)\", globals=globals(), number=num, repeat=reps)\n    gradient_shift.append([num_params, min(t) / num])\n\n    # backprop\n    t = timeit.repeat(\"qml.grad(qnode_backprop)(params)\", globals=globals(), number=num, repeat=reps)\n    gradient_backprop.append([num_params, min(t) / num])\n\ngradient_shift = np.array(gradient_shift).T\ngradient_backprop = np.array(gradient_backprop).T\nforward_shift = np.array(forward_shift).T\nforward_backprop = np.array(forward_backprop).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now import matplotlib, and plot the results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.style.use(\"bmh\")\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 4))\n\nax.plot(*gradient_shift, '.-', label=\"Parameter-shift\")\nax.plot(*gradient_backprop, '.-', label=\"Backprop\")\nax.set_ylabel(\"Time (s)\")\nax.set_xlabel(\"Number of parameters\")\nax.legend()\n\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```{=html}\n<br>\n```\nWe can see that the computational time for the parameter-shift rule\nincreases with increasing number of parameters, as expected, whereas the\ncomputational time for backpropagation appears much more constant, with\nperhaps a minute linear increase with $p$. Note that the plots are not\nperfectly linear, with some \\'bumpiness\\' or noisiness. This is likely\ndue to low-level operating system jitter, and other environmental\nfluctuations\\-\\--increasing the number of repeats can help smooth out\nthe plot.\n\nFor a better comparison, we can scale the time required for computing\nthe quantum gradients against the time taken for the corresponding\nforward pass:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "gradient_shift[1] /= forward_shift[1, 1:]\ngradient_backprop[1] /= forward_backprop[1, 1:]\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 4))\n\nax.plot(*gradient_shift, '.-', label=\"Parameter-shift\")\nax.plot(*gradient_backprop, '.-', label=\"Backprop\")\n\n# perform a least squares regression to determine the linear best fit/gradient\n# for the normalized time vs. number of parameters\nx = gradient_shift[0]\nm_shift, c_shift = np.polyfit(*gradient_shift, deg=1)\nm_back, c_back = np.polyfit(*gradient_backprop, deg=1)\n\nax.plot(x, m_shift * x + c_shift, '--', label=f\"{m_shift:.2f}p{c_shift:+.2f}\")\nax.plot(x, m_back * x + c_back, '--', label=f\"{m_back:.2f}p{c_back:+.2f}\")\n\nax.set_ylabel(\"Normalized time\")\nax.set_xlabel(\"Number of parameters\")\nax.set_xscale(\"log\")\nax.set_yscale(\"log\")\nax.legend()\n\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```{=html}\n<br>\n```\nWe can now see clearly that there is constant overhead for\nbackpropagation with `default.qubit`, but the parameter-shift rule\nscales as $\\sim 2p$.\n\nAbout the author\n================\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}