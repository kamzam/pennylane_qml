{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# This cell is added by sphinx-gallery\n# It can be customized to whatever you like\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using JAX with PennyLane\n========================\n\n::: {.meta}\n:property=\\\"og:description\\\": Learn how to use JAX with PennyLane.\n:property=\\\"og:image\\\": <https://pennylane.ai/qml/_images/jax.png>\n:::\n\n::: {.related}\ntutorial\\_qubit\\_rotation Basic tutorial: qubit rotation tutorial\\_vqe A\nbrief overview of VQE tutorial\\_vqt Variational Quantum Thermalizer\n:::\n\n*Author: Chase Roberts --- Posted: 12 April 2021. Last updated: 12 April\n2021.*\n\nJAX is an incredibly powerful scientific computing library that has been\ngaining traction in both the physics and deep learning communities.\nWhile JAX was originally designed for classical machine learning (ML),\nmany of its transformations are also useful for quantum machine learning\n(QML), and can be used directly with PennyLane.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](../demonstrations/jax_logo/jax.png){.align-center width=\"50.0%\"}\n\nIn this tutorial, we\\'ll go over a number of JAX transformations and\nshow how you can use them to build and optimize quantum circuits. We\\'ll\nshow examples of how to do gradient descent with `jax.grad`, run quantum\ncircuits in parallel using `jax.vmap`, compile and optimize simulations\nwith `jax.jit`, and control and seed the random nature of quantum\ncomputer simulations with `jax.random`. By the end of this tutorial you\nshould feel just as comfortable transforming quantum computing programs\nwith JAX as you do transforming your neural networks.\n\nIf this is your first time reading PennyLane code, we recommend going\nthrough the\n`basic tutorial </demos/tutorial_qubit_rotation>`{.interpreted-text\nrole=\"doc\"} first. It\\'s all in vanilla NumPy, so you should be able to\neasily transfer what you learn to JAX when you come back.\n\nWith that said, we begin by importing PennyLane, JAX, the JAX-provided\nversion of NumPy and set up a two-qubit device for computations. We\\'ll\nbe using the `default.qubit` device for the first part of this tutorial.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Added to silence some warnings.\nfrom jax.config import config\nconfig.update(\"jax_enable_x64\", True)\n\nimport jax\nimport jax.numpy as jnp\nimport pennylane as qml\n\ndev = qml.device(\"default.qubit\", wires=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\\'s start with a simple example circuit that generates a two-qubit\nentangled state, then evaluates the expectation value of the Pauli-Z\noperator on the first wire.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@qml.qnode(dev, interface=\"jax\")\ndef circuit(param):\n    # These two gates represent our QML model. \n    qml.RX(param, wires=0)\n    qml.CNOT(wires=[0, 1])\n\n    # The expval here will be the \"cost function\" we try to minimize.\n    # Usually, this would be defined by the problem we want to solve,\n    # but for this example we'll just use a single PauliZ.\n    return qml.expval(qml.PauliZ(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now execute the circuit just like any other python function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"Result: {repr(circuit(0.123))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice that the output of the circuit is a JAX `DeviceArray`. In fact,\nwhen we use the `default.qubit` device, the entire computation is done\nin JAX, so we can use all of the JAX tools out of the box!\n\nNow let\\'s move on to an example of a transformation. The code we wrote\nabove is entirely differentiable, so let\\'s calculate its gradient with\n`jax.grad`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\nGradient Descent\")\nprint(\"---------------\")\n\n# We use jax.grad here to transform our circuit method into one\n# that calcuates the gradient of the output relative to the input.\n\ngrad_circuit = jax.grad(circuit)\nprint(f\"grad_circuit(jnp.pi / 2): {grad_circuit(jnp.pi / 2):0.3f}\")\n\n# We can then use this grad_circuit function to optimize the parameter value\n# via gradient descent.\nparam = 0.123 # Some initial value. \n\nprint(f\"Initial param: {param:0.3f}\")\nprint(f\"Initial cost: {circuit(param):0.3f}\")\n\nfor _ in range(100): # Run for 100 steps.\n    param -= grad_circuit(param) # Gradient-descent update.\n\nprint(f\"Tuned param: {param:0.3f}\")\nprint(f\"Tuned cost: {circuit(param):0.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And that\\'s QML in a nutshell! If you\\'ve done classical machine\nlearning before, the above training loop should feel very familiar to\nyou. The only difference is that we used a quantum computer (or rather,\na simulation of one) as part of our model and cost calculation. In the\nend, almost all QML problems involve tuning some parameters and\nminimizing some cost function, just like classical ML. While classical\nML focuses on learning classical systems like language or vision, QML is\nmost useful for learning about quantum systems. For example,\n`finding chemical ground states </demos/tutorial_vqe>`{.interpreted-text\nrole=\"doc\"} or learning to\n`sample thermal energy states </demos/tutorial_vqt>`{.interpreted-text\nrole=\"doc\"}.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Batching and Evolutionary Strategies\n====================================\n\n![](../demonstrations/jax_logo/jaxvmap.png){.align-center width=\"50.0%\"}\n\nWe just showed how we can use gradient methods to learn a parameter\nvalue, but on real quantum computing hardware, calculating gradients can\nbe really expensive and noisy. Another approach is to use [evolutionary\nstrategies](https://arxiv.org/abs/2012.00101) (ES) to learn these\nparameters. Here, we will be using the `jax.vmap`\n[transform](https://jax.readthedocs.io/en/latest/jax.html#jax.vmap) to\nmake running batches of circuits much easier. `vmap` essentially\ntransforms a single quantum computer into multiple running in parallel!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n\\nBatching and Evolutionary Strategies\")\nprint(\"------------------------------------\")\n\n# Create a vectorized version of our original circuit.\nvcircuit = jax.vmap(circuit)\n\n# Now, we call the ``vcircuit`` with multiple parameters at once and get back a\n# batch of expectations.\n# This examples runs 3 quantum circuits in parallel.\nbatch_params = jnp.array([1.02, 0.123, -0.571])\n\nbatched_results = vcircuit(batch_params)\nprint(f\"Batched result: {batched_results}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\\'s now set up our ES training loop. The idea is pretty simple.\nFirst, we calculate the expected values of each of our parameters. The\ncost values then determine the \\\"weight\\\" of that example. The lower the\ncost, the larger the weight. These batches are then used to generate a\nnew set of parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Needed to do randomness with JAX.\n# For more info on how JAX handles randomness, see the documentation.\n# https://jax.readthedocs.io/en/latest/jax.random.html\nkey = jax.random.PRNGKey(0)\n\n# Generate our first set of samples.\nparams = jax.random.normal(key, (100,))\nmean = jnp.average(params)\nvar = 1.0\nprint(f\"Initial value: {mean:0.3f}\")\nprint(f\"Initial cost: {circuit(mean):0.3f}\")\n\nfor _ in range(200):\n    # In this line, we run all 100 circuits in parallel.\n    costs = vcircuit(params)\n\n    # Use exp(-x) here since the costs could be negative.\n    weights = jnp.exp(-costs) \n    mean = jnp.average(params, weights=weights)\n\n    # We decrease the variance as we converge to a solution.\n    var = var * 0.97\n\n    # Split the PRNGKey to generate a new set of random samples.\n    key, split = jax.random.split(key)\n    params = jax.random.normal(split, (100,)) * var + mean\n\nprint(f\"Final value: {mean:0.3f}\")\nprint(f\"Final cost: {circuit(mean):0.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "How to use jax.jit: Compiling Circuit Execution\n===============================================\n\n![](../demonstrations/jax_logo/jaxjit.png){.align-center width=\"50.0%\"}\n\nJAX is built on top of [XLA](https://www.tensorflow.org/xla), a powerful\nnumerics library that can optimize and cross compile computations to\ndifferent hardware, including CPUs, GPUs, etc. JAX can compile its\ncomputation to XLA via the `jax.jit`\n[transform.](https://jax.readthedocs.io/en/latest/jax.html?highlight=jit#jax.jit)\n\nWhen compiling an XLA program, the compiler will do several rounds of\noptimization passes to enhance the performance of the computation.\nBecause of this compilation overhead, you\\'ll generally find the first\ntime calling the function to be slow, but all subsequent calls are much,\nmuch faster. You\\'ll likely want to do it if you\\'re running the same\ncircuit over and over but with different parameters, like you would find\nin almost all variational quantum algorithms.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n\\nJit Example\")\nprint(\"-----------\")\n\n@qml.qnode(dev, interface=\"jax\")\ndef circuit(param):\n    qml.RX(param, wires=0)\n    qml.CNOT(wires=[0, 1])\n    return qml.expval(qml.PauliZ(0))\n\n# Compiling your circuit with JAX is very easy, just add jax.jit!\njit_circuit = jax.jit(circuit)\n\nimport time\n\n# No jit.\nstart = time.time()\n# JAX runs async, so .block_until_ready() blocks until the computation\n# is actually finished. You'll only need to use this if you're doing benchmarking.\ncircuit(0.123).block_until_ready()\nno_jit_time = time.time() - start\n\n# First call with jit.\nstart = time.time()\njit_circuit(0.123).block_until_ready()\nfirst_time = time.time() - start\n\n# Second call with jit.\nstart = time.time()\njit_circuit(0.123).block_until_ready()\nsecond_time = time.time() - start\n\n\nprint(f\"No jit time: {no_jit_time:0.4f} seconds\")\n# Compilation overhead will make the first call slower than without jit...\nprint(f\"First run time: {first_time:0.4f} seconds\")\n# ... but the second run time is >100x faster than the first!\nprint(f\"Second run time: {second_time:0.4f} seconds\")\n\n\n# You can see that for the cost of some compilation overhead, we can\n# greatly increase our performance of our simulation by orders of magnitude."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Shots and Sampling with JAX\n===========================\n\nJAX was designed to enable experiments to be as repeatable as possible.\nBecause of this, JAX requires us to seed all randomly generated values\n(as you saw in the above batching example). Sadly, the universe doesn\\'t\nallow us to seed real quantum computers, so if we want our JAX to mimic\na real device, we\\'ll have to handle randomness ourselves.\n\nTo learn more about how JAX handles randomness, visit their\n[documentation\nsite.](https://jax.readthedocs.io/en/latest/jax.random.html)\n\n::: {.note}\n::: {.title}\nNote\n:::\n\nThis example only applies if you are using `jax.jit`. Otherwise,\nPennyLane automatically seeds and resets the random-number-generator for\nyou on each call.\n:::\n\nTo set the random number generating key, you\\'ll have to pass the\n`jax.random.PRNGKey` when constructing the device. Because of this, if\nyou want to use `jax.jit` with randomness, the device construction will\nhave to happen within that jitted method.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n\\nRandomness\")\nprint(\"----------\")\n\n# Let's create our circuit with randomness and compile it with jax.jit.\n@jax.jit\ndef circuit(key, param):\n    # Notice how the device construction now happens within the jitted method.\n    # Also note the added '.jax' to the device path.\n    dev = qml.device(\"default.qubit.jax\", wires=2, shots=10, prng_key=key)\n\n    # Now we can create our qnode within the circuit function.\n    @qml.qnode(dev, interface=\"jax\", diff_method=None)\n    def my_circuit():\n        qml.RX(param, wires=0)\n        qml.CNOT(wires=[0, 1])\n        return qml.sample(qml.PauliZ(0))\n    return my_circuit()\n\nkey1 = jax.random.PRNGKey(0)\nkey2 = jax.random.PRNGKey(1)\n\n# Notice that the first two runs return exactly the same results,\nprint(f\"key1: {circuit(key1, jnp.pi/2)}\")\nprint(f\"key1: {circuit(key1, jnp.pi/2)}\")\n\n# The second run has different results.\nprint(f\"key2: {circuit(key2, jnp.pi/2)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Closing Remarks\n===============\n\nBy now, using JAX with PennyLane should feel very natural. They\ncomplement each other very nicely; JAX with its powerful transforms, and\nPennyLane with its easy access to quantum computers. We\\'re still in\nearly days of development, but we hope to continue to grow our ecosystem\naround JAX, and by extension, grow JAX into quantum computing and\nquantum machine learning. The future looks bright for this field, and\nwe\\'re excited to see what you build!\n\nAbout the author\n================\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}