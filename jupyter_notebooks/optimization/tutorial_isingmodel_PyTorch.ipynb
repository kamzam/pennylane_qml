{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# This cell is added by sphinx-gallery\n# It can be customized to whatever you like\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3-qubit Ising model in PyTorch {#isingmodel_PyTorch}\n==============================\n\n::: {.meta}\n:property=\\\"og:description\\\": This demonstration uses the PyTorch\ninterface of PennyLane to optimize a 3-qubit Ising model.\n:property=\\\"og:image\\\":\n<https://pennylane.ai/qml/_images/isingspins.png>\n:::\n\n::: {.related}\ntutorial\\_state\\_preparation Training a quantum circuit with PyTorch\npytorch\\_noise PyTorch and noisy devices\n:::\n\n*Author: Aroosa Ijaz --- Posted: 16 October 2019. Last updated: 26\nOctober 2020.*\n\nThe interacting spins with variable coupling strengths of an [Ising\nmodel](https://en.wikipedia.org/wiki/Ising_model) can be used to\nsimulate various machine learning concepts like [Hopfield\nnetworks](https://en.wikipedia.org/wiki/Hopfield_network) and [Boltzmann\nmachines](https://en.wikipedia.org/wiki/Boltzmann_machine) ([Schuld &\nPetruccione (2018)](https://www.springer.com/gp/book/9783319964232)).\nThey also closely imitate the underlying mathematics of a subclass of\ncomputational problems called [Quadratic Unconstrained Binary\nOptimization\n(QUBO)](https://en.wikipedia.org/wiki/Quadratic_unconstrained_binary_optimization)\nproblems.\n\nIsing models are commonly encountered in the subject area of adiabatic\nquantum computing. Quantum annealing algorithms (for example, as\nperformed on a D-wave system) are often used to find low-energy\nconfigurations of Ising problems. The optimization landscape of the\nIsing model is non-convex, which can make finding global minima\nchallenging. In this tutorial, we get a closer look at this phenomenon\nby applying gradient descent techniques to a toy Ising model.\u00a0\n\nPennyLane implementation\n------------------------\n\nThis basic tutorial optimizes a 3-qubit Ising model using the PennyLane\n`default.qubit` device with PyTorch. In the absence of external fields,\nthe Hamiltonian for this system is given by:\n\n$$H=-\\sum_{<i,j>} J_{ij} \\sigma_i \\sigma_{j},$$\n\nwhere each spin can be in the +1 or -1 spin state and $J_{ij}$ are the\nnearest-neighbour coupling strengths.\n\nFor simplicity, the first spin can be assumed to be in the \\\"up\\\" state\n(+1 eigenstate of Pauli-Z operator) and the coupling matrix can be set\nto $J = [1,-1]$. The rotation angles for the other two spins are then\noptimized so that the energy of the system is minimized for the given\ncouplings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom torch.autograd import Variable\nimport pennylane as qml\nfrom pennylane import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A three-qubit quantum circuit is initialized to represent the three\nspins:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dev = qml.device(\"default.qubit\", wires=3)\n\n@qml.qnode(dev, interface=\"torch\") \ndef circuit(p1, p2):\n    # We use the general Rot(phi,theta,omega,wires) single-qubit operation\n    qml.Rot(p1[0], p1[1], p1[2], wires=1)\n    qml.Rot(p2[0], p2[1], p2[2], wires=2)\n    return [qml.expval(qml.PauliZ(i)) for i in range(3)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The cost function to be minimized is defined as the energy of the spin\nconfiguration:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def cost(var1, var2):\n    # the circuit function returns a numpy array of Pauli-Z expectation values\n    spins = circuit(var1, var2)\n\n    # the expectation value of Pauli-Z is +1 for spin up and -1 for spin down\n    energy = -(1 * spins[0] * spins[1]) - (-1 * spins[1] * spins[2])\n    return energy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sanity check\n============\n\nLet\\'s test the functions above using the\n$[s_1, s_2, s_3] = [1, -1, -1]$ spin configuration and the given\ncoupling matrix. The total energy for this Ising model should be:\n\n$$H = -1(J_1 s_1 \\otimes s_2 + J_2 s_2 \\otimes s3) = 2$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "test1 = torch.tensor([0, np.pi, 0])\ntest2 = torch.tensor([0, np.pi, 0])\n\ncost_check = cost(test1, test2)\nprint(\"Energy for [1, -1, -1] spin configuration:\", cost_check)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Random initialization\n=====================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(56)\np1 = Variable((np.pi * torch.rand(3, dtype=torch.float64)), requires_grad=True)\np2 = Variable((np.pi * torch.rand(3, dtype=torch.float64)), requires_grad=True)\n\nvar_init = [p1, p2]\ncost_init = cost(p1, p2)\n\nprint(\"Randomly initialized angles:\")\nprint(p1)\nprint(p2)\nprint(\"Corresponding cost before optimization:\")\nprint(cost_init)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optimization\n============\n\nNow we use the PyTorch gradient descent optimizer to minimize the cost:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "opt = torch.optim.SGD(var_init, lr=0.1)\n\ndef closure():\n    opt.zero_grad()\n    loss = cost(p1, p2)\n    loss.backward()\n    return loss\n\nvar_pt = [var_init]\ncost_pt = [cost_init]\nx = [0]\n\nfor i in range(100):\n    opt.step(closure)\n    if (i + 1) % 5 == 0:\n        x.append(i)\n        p1n, p2n = opt.param_groups[0][\"params\"]\n        costn = cost(p1n, p2n)\n        var_pt.append([p1n, p2n])\n        cost_pt.append(costn)\n\n        # for clarity, the angles are printed as numpy arrays\n        print(\"Energy after step {:5d}: {: .7f} | Angles: {}\".format(\n            i+1, costn, [p1n.detach().numpy(), p2n.detach().numpy()]),\"\\n\"\n        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.note}\n::: {.title}\nNote\n:::\n\nWhen using the *PyTorch* optimizer, keep in mind that:\n\n1.  `loss.backward()` computes the gradient of the cost function with\n    respect to all parameters with `requires_grad=True`.\n2.  `opt.step()` performs the parameter update based on this *current*\n    gradient and the learning rate.\n3.  `opt.zero_grad()` sets all the gradients back to zero. It\\'s\n    important to call this before `loss.backward()` to avoid the\n    accumulation of gradients from multiple passes.\n\nHence, its standard practice to define the `closure()` function that\nclears up the old gradient, evaluates the new gradient and passes it\nonto the optimizer in each step.\n:::\n\nThe minimum energy is -2 for the spin configuration\n$[s_1, s_2, s_3] = [1, 1, -1]$ which corresponds to\n$(\\phi, \\theta, \\omega) = (0, 0, 0)$ for the second spin and\n$(\\phi, \\theta, \\omega) = (0, \\pi, 0)$ for the third spin. Note that\ngradient descent optimization might not find this global minimum due to\nthe non-convex cost function, as is shown in the next section.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "p1_final, p2_final = opt.param_groups[0][\"params\"]\nprint(\"Optimized angles:\")\nprint(p1_final)\nprint(p2_final)\nprint(\"Final cost after optimization:\")\nprint(cost(p1_final, p2_final))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(6, 4))\n\n# Enable processing the Torch trainable tensors\nwith torch.no_grad():\n    plt.plot(x, cost_pt, label = 'global minimum')\n    plt.xlabel(\"Optimization steps\")\n    plt.ylabel(\"Cost / Energy\")\n    plt.legend()\n    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Local minimum\n=============\n\nIf the spins are initialized close to the local minimum of zero energy,\nthe optimizer is likely to get stuck here and never find the global\nminimum at -2.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(9)\np3 = Variable((np.pi*torch.rand(3, dtype = torch.float64)), requires_grad = True)\np4 = Variable((np.pi*torch.rand(3, dtype = torch.float64)), requires_grad = True)\n\nvar_init_loc = [p3, p4]\ncost_init_loc = cost(p3, p4)\n\nprint(\"Corresponding cost before optimization:\")\nprint(cost_init_loc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "opt = torch.optim.SGD(var_init_loc, lr = 0.1)\n\ndef closure():\n    opt.zero_grad()\n    loss = cost(p3, p4)\n    loss.backward()\n    return loss\n\nvar_pt_loc = [var_init_loc]\ncost_pt_loc = [cost_init_loc]\n\nfor j in range(100):\n    opt.step(closure)\n    if (j + 1) % 5 == 0:\n        p3n, p4n = opt.param_groups[0]['params']\n        costn = cost(p3n, p4n)\n        var_pt_loc.append([p3n, p4n])\n        cost_pt_loc.append(costn)\n\n        # for clarity, the angles are printed as numpy arrays\n        print('Energy after step {:5d}: {: .7f} | Angles: {}'.format(\n            j+1, costn, [p3n.detach().numpy(), p4n.detach().numpy()]),\"\\n\"\n        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(6, 4))\n\n# Enable processing the Torch trainable tensors\nwith torch.no_grad():\n    plt.plot(x, cost_pt_loc, 'r', label = 'local minimum')\n    plt.xlabel(\"Optimization steps\")\n    plt.ylabel(\"Cost / Energy\")\n    plt.legend()\n    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| \n\nTry it yourself! Download and run this file with different\ninitialization parameters and see how the results change.\n\nFurther reading\n===============\n\n1\\. Maria Schuld and Francesco Petruccione. \\\"Supervised Learning with\nQuantum Computers.\\\" Springer, 2018.\n\n2\\. Andrew Lucas. \\\"Ising formulations of many NP problems.\\\"\n[arXiv:1302.5843](https://arxiv.org/pdf/1302.5843), 2014.\n\n3\\. Gary Kochenberger et al. \\\"The Unconstrained Binary Quadratic\nProgramming Problem: A Survey.\\\" [Journal of Combinatorial\nOptimization](https://link.springer.com/article/10.1007/s10878-014-9734-0),\n2014.\n\nAbout the author\n================\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}