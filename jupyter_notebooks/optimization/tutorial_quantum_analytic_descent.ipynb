{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# This cell is added by sphinx-gallery\n# It can be customized to whatever you like\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quantum analytic descent {#quantum_analytic_descent}\n========================\n\n::: {.meta}\n:property=\\\"og:description\\\": Implement the Quantum analytic descent\nalgorithm for VQE. :property=\\\"og:image\\\":\n<https://pennylane.ai/qml/_images/flowchart.png>\n:::\n\n::: {.related}\ntutorial\\_vqe A brief overview of VQE\ntutorial\\_quantum\\_natural\\_gradient Quantum natural gradient\ntutorial\\_rotoselect Quantum circuit structure learning\ntutorial\\_stochastic\\_parameter\\_shift The stochastic parameter-shift\nrule\n:::\n\n*Authors: Elies Gil-Fuster, David Wierichs (Xanadu Residents) ---\nPosted: 30 June 2021. Last updated: 18 November 2021*\n\nOne of the main problems of many-body physics is that of finding the\nground state and ground state energy of a given Hamiltonian. [The\nVariational Quantum Eigensolver\n(VQE)](https://pennylane.ai/qml/demos/tutorial_vqe.html) combines smart\ncircuit design with gradient-based optimization to solve this task.\nSeveral practical demonstrations have shown how near-term quantum\ndevices may be suitable for VQE and other variational quantum\nalgorithms. One issue for such an approach, though, is that the\noptimization landscape is non-convex, so reaching a good enough local\nminimum quickly requires hundreds or thousands of update steps. This is\nproblematic because computing gradients of the cost function on a\nquantum computer is inefficient when it comes to circuits with many\nparameters.\n\nAt the same time, we have a good understanding of the *local* shape of\nthe cost landscape around any reference point. Cashing in on this, the\nauthors of the Quantum Analytic Descent paper propose an algorithm that\nconstructs a classical model which approximates the landscape, so that\nthe gradients can be calculated on a classical computer, which is much\ncheaper. In order to build the classical model, we need to use the\nquantum device to evaluate the cost function on (a) a reference point\n$\\boldsymbol{\\theta}_0$, and (b) a number of points shifted away from\n$\\boldsymbol{\\theta}_0$. With the cost values at these points, we can\nbuild the classical model that approximates the landscape.\n\nIn this demo, you will learn how to implement Quantum Analytic Descent\nusing PennyLane. In addition, you will look under the hood of the\nconstructed models and the optimization steps carried out by the\nalgorithm. So: sit down, relax, and enjoy your optimization!\n\n| \n\n![Optimization progress with Quantum Analytic\nDescent.](../demonstrations/quantum_analytic_descent/xkcd.png){.align-center\nwidth=\"50.0%\"}\n\nVQEs give rise to trigonometric cost functions\n----------------------------------------------\n\nWhen we talk about VQEs we have a quantum circuit with $n$ qubits in\nmind, which are typically initialized in the base state $|0\\rangle$. The\nbody of the circuit is a *variational form* $V(\\boldsymbol{\\theta})$ \\--\na fixed architecture of quantum gates parametrized by an array of\nreal-valued parameters $\\boldsymbol{\\theta}\\in\\mathbb{R}^m$. After the\nvariational form, the circuit ends with the measurement of a chosen\nobservable $\\mathcal{M}$, based on the problem we are trying to solve.\n\nThe idea in VQE is to fix a variational form such that the expected\nvalue of the measurement relates to the energy of an interesting\nHamiltonian:\n\n$$E(\\boldsymbol{\\theta}) = \\langle 0|V^\\dagger(\\boldsymbol{\\theta})\\mathcal{M}V(\\boldsymbol{\\theta})|0\\rangle.$$\n\nWe want to find the lowest possible energy the system can attain; this\ncorresponds to running an optimization program to find the\n$\\boldsymbol{\\theta}$ that minimizes the function above.\n\nIf the gates in the variational form are restricted to be Pauli\nrotations, then the cost function is a sum of *multilinear trigonometric\nterms* in each of the parameters. That\\'s a scary sequence of words!\nWhat it means is that if we look at $E(\\boldsymbol{\\theta})$ but we\nfocus only on one of the parameters, say $\\theta_i$, then we can write\nthe functional dependence as a linear combination of three functions:\n$1$, $\\sin(\\theta_i)$, and $\\cos(\\theta_i)$. That is, for each parameter\n$\\theta_i$ there exist $a_i$, $b_i$, and $c_i$ such that the cost can be\nwritten as\n\n$$E(\\boldsymbol{\\theta}) = a_i + b_i\\sin(\\theta_i) + c_i\\cos(\\theta_i).$$\n\nAll parameters but $\\theta_i$ are absorbed in the coefficients $a_i$,\n$b_i$ and $c_i$. Another technique using this structure of\n$E(\\boldsymbol{\\theta})$ are the Rotosolve/Rotoselect algorithms for\nwhich there also is [a PennyLane\ndemo](https://pennylane.ai/qml/demos/tutorial_rotoselect.html).\n\nLet\\'s look at a toy example to illustrate this structure of the cost\nfunction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pennylane as qml\nfrom pennylane import numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nnp.random.seed(0)\n\n# Create a device with 2 qubits.\ndev = qml.device(\"default.qubit\", wires=2)\n\n# Define the variational form V and observable M and combine them into a QNode.\n@qml.qnode(dev, diff_method=\"parameter-shift\", max_diff=2)\ndef circuit(parameters):\n    qml.RX(parameters[0], wires=0)\n    qml.RX(parameters[1], wires=1)\n    return qml.expval(qml.PauliZ(0) @ qml.PauliZ(1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us now look at how the energy value depends on each of the two\nparameters alone. For that, we just fix one parameter and show the cost\nwhen varying the other one:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Create 1D sweeps through parameter space with the other parameter fixed.\nnum_samples = 50\n\n# Fix a parameter position.\nparameters = np.array([3.3, 0.5], requires_grad=True)\n\ntheta_func = np.linspace(0, 2 * np.pi, num_samples)\nC1 = [circuit(np.array([theta, parameters[1]])) for theta in theta_func]\nC2 = [circuit(np.array([parameters[0], theta])) for theta in theta_func]\n\n# Show the sweeps.\nfig, ax = plt.subplots(1, 1, figsize=(4, 3))\nax.plot(theta_func, C1, label=\"$E(\\\\theta, 0.5)$\", color=\"r\")\nax.plot(theta_func, C2, label=\"$E(3.3, \\\\theta)$\", color=\"orange\")\nax.set_xlabel(\"$\\\\theta$\")\nax.set_ylabel(\"$E$\")\nax.legend()\nplt.tight_layout()\n\n# Create a 2D grid and evaluate the energy on the grid points.\n# We cut out a part of the landscape to increase clarity.\nX, Y = np.meshgrid(theta_func, theta_func)\nZ = np.zeros_like(X)\nfor i, t1 in enumerate(theta_func):\n    for j, t2 in enumerate(theta_func):\n        # Cut out the viewer-facing corner\n        if (2 * np.pi - t2) ** 2 + t1 ** 2 > 4:\n            Z[i, j] = circuit([t1, t2])\n        else:\n            X[i, j] = Y[i, j] = Z[i, j] = np.nan\n\n# Show the energy landscape on the grid.\nfig, ax = plt.subplots(1, 1, subplot_kw={\"projection\": \"3d\"}, figsize=(4, 4))\nsurf = ax.plot_surface(X, Y, Z, label=\"$E(\\\\theta_1, \\\\theta_2)$\", alpha=0.7, color=\"#209494\")\nline1 = ax.plot(\n    [parameters[1]] * num_samples,\n    theta_func,\n    C1,\n    label=\"$E(\\\\theta_1, \\\\theta_2^{(0)})$\",\n    color=\"r\",\n    zorder=100,\n)\nline2 = ax.plot(\n    theta_func,\n    [parameters[0]] * num_samples,\n    C2,\n    label=\"$E(\\\\theta_1^{(0)}, \\\\theta_2)$\",\n    color=\"orange\",\n    zorder=100,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Of course this is an overly simplified example, but the key take-home\nmessage so far is: *if the variational parameters feed into Pauli\nrotations, the energy landscape is a multilinear combination of\ntrigonometric functions*. What is a good thing about trigonometric\nfunctions? That\\'s right! We have studied them since high school and\nknow how their graphs look.\n\nThe QAD strategy\n================\n\nBy now we know how the energy landscape looks for a small example.\nScaling this up to more parameters would quickly become unfeasible\nbecause we need to query a quantum computer for every combination of\nparameter values. The secret ingredient of this sauce is that we only\nneed to build an approximate classical model. Using an approximate\nclassical model has one feature and one bug. The feature: it is cheap to\nconstruct. The bug: well, it\\'s only approximate, so we cannot rely on\nit fully. And one extra feature (you didn\\'t see that coming, did you?):\nif the reference point about which we build the classical model is a\ntrue local minimum, then it will be a local minimum of the classical\nmodel too. And that is the key! Given a reference point, we use the\nclassical model to find a point that\\'s closer to the true minimum, and\nthen use that point as reference for a new model! This is what is called\nQuantum Analytic Descent (QAD), and if you are fine not knowing yet what\nall the symbols mean, here\\'s its pseudo-algorithm:\n\n1.  Set an initial reference point $\\boldsymbol{\\theta}_0$.\n2.  Build the model\n    $\\hat{E}(\\boldsymbol{\\theta})\\approx E(\\boldsymbol{\\theta}_0+\\boldsymbol{\\theta})$\n    at this point.\n3.  Find the minimum $\\boldsymbol{\\theta}_\\text{min}$ of the model.\n4.  Set $\\boldsymbol{\\theta}_0+\\boldsymbol{\\theta}_\\text{min}$ as the\n    new reference point $\\boldsymbol{\\theta}_0$, go back to Step 2.\n5.  After convergence or a fixed number of models built, output the last\n    minimum\n    $\\boldsymbol{\\theta}_\\text{opt}=\\boldsymbol{\\theta}_0+\\boldsymbol{\\theta}_\\text{min}$.\n\nComputing a classical model\n===========================\n\nKnowing how the cost looks when restricted to only one parameter (see\nthe plot above), nothing keeps us in theory from constructing a perfect\nclassical model. The only thing we need to do is write down a general\nmultilinear trigonometric polynomial and determine its coefficients.\nSimple, right? Well, for $m$ parameters, there would be $3^m$\ncoefficients to estimate, which gives us the ever-dreaded exponential\nscaling. Although conceptually simple, building an exact model would\nrequire exponentially many resources, and that\\'s a no-go. What can we\ndo, then? The authors of QAD propose building an imperfect model. This\nmakes *all* the difference\\-\\--they use a classical model that is\naccurate only in a region close to a given reference point, and that\ndelivers good results for the optimization!\n\nFunction expansions\n-------------------\n\nWhat do we usually do when we want to approximate something in a region\nnear to a reference point? Correct! We use a Taylor expansion! But what\nif we told you there is a better option for the case at hand? In the QAD\npaper, the authors state that a *trigonometric expansion* up to second\norder is already a sound model candidate. Let\\'s recap such expansions.\n\n::: {.admonition}\nTaylor expansion vs. Trigonometric expansion\n\nIn spirit, a trigonometric expansion and a Taylor expansion are not that\ndifferent: both are linear combinations of some basis functions, where\nthe coefficients of the sum take very specific values usually related to\nthe derivatives of the function we want to approximate. The difference\nbetween Taylor\\'s and a trigonometric expansion is mainly what basis of\nfunctions we take. In Calculus I we learned that a Taylor series in one\nvariable $x$ uses the integer powers of the variable namely\n$\\{1, x, x^2, x^3, \\ldots\\}$, in short $\\{x^n\\}_{n\\in\\mathbb{N}}$:\n\n$$f_\\text{Taylor}(x) = \\sum c_n(x-x_0)^n.$$\n\nA trigonometric expansion instead uses a different basis, also for one\nvariable: $\\{1, \\sin(x), \\cos(x), \\sin(2x), \\cos(2x), \\ldots\\}$, which\nwe could call the set of trigonometric monomials with integer frequency,\nor in short $\\{\\sin(nx),\\cos(nx)\\}_{n\\in\\mathbb{N}}$:\n\n$$f_\\text{Trig}(x) = \\sum a_n \\cos(n(x-x_0))+ b_n \\sin(n(x-x_0)).$$\n\nFor higher-dimensional variables we have to take products of the basis\nfunctions of each coordinate, i.e., of monomials or trigonometric\nmonomials respectively. This does lead to an exponentially increasing\nnumber of terms, but if we chop the series soon enough it will not get\ntoo much out of hand. The proposal here is to only go up to second order\nterms, so we are safe.\n:::\n\nExpanding in adapted trigonometric polynomials\n----------------------------------------------\n\nOne important aspect in which trigonometric series differ from regular\nexpansions is that there is not a clear separation between what terms\ncontribute to each order of the expansion (due to the fact that all\nderivatives of sine and cosine are non-zero in general). Because of\nthis, we group the terms by their leading order contribution, and in the\nfollowing table write them next to their non-trigonometric analogues.\nAll chosen trigonometric monomials have leading order coefficient $1$\nand they all differ in their leading order contribution.\n\n  -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  Order   Trigonometric monomial                                                                                                                                                                                                                 Taylor\n                                                                                                                                                                                                                                                 monomial\n  ------- -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- --------------\n  0       $A(\\boldsymbol{\\theta})= \\prod_{i=1}^m \\cos\\left(\\frac{\\theta_i}{2}\\right)^2$                                                                                                                                                          $1$\n\n  1       $B_k(\\boldsymbol{\\theta}) = 2\\cos\\left(\\frac{\\theta_k}{2}\\right)\\sin\\left(\\frac{\\theta_k}{2}\\right)\\prod_{i\\neq k} \\cos\\left(\\frac{\\theta_i}{2}\\right)^2$                                                                              $x_k$\n\n  2       $C_k(\\boldsymbol{\\theta}) = 2\\sin\\left(\\frac{\\theta_k}{2}\\right)^2\\prod_{i\\neq k} \\cos\\left(\\frac{\\theta_i}{2}\\right)^2$                                                                                                               $x_k^2$\n\n  2       $D_{kl}(\\boldsymbol{\\theta}) = 4\\sin\\left(\\frac{\\theta_k}{2}\\right)\\cos\\left(\\frac{\\theta_k}{2}\\right)\\sin\\left(\\frac{\\theta_l}{2}\\right)\\cos\\left(\\frac{\\theta_l}{2}\\right)\\prod_{i\\neq k,l} \\cos\\left(\\frac{\\theta_i}{2}\\right)^2$   $x_kx_l$\n  -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nThose are really large terms compared to a Taylor series! However, you\nmay have noticed all of those terms have large parts in common. Indeed,\nwe can rewrite the longer ones in a shorter way which is more decent to\nlook at:\n\n$$\\begin{aligned}\nB_k(\\boldsymbol{\\theta}) &= 2\\tan\\left(\\frac{\\theta_k}{2}\\right)A(\\boldsymbol{\\theta})\\\\\nC_k(\\boldsymbol{\\theta}) &= 2\\tan\\left(\\frac{\\theta_k}{2}\\right)^2 A(\\boldsymbol{\\theta})\\\\\nD_{kl}(\\boldsymbol{\\theta}) &= 4\\tan\\left(\\frac{\\theta_k}{2}\\right)\\tan\\left(\\frac{\\theta_l}{2}\\right)A(\\boldsymbol{\\theta})\n\\end{aligned}$$\n\nWith that, we know what type of terms we should expect to encounter in\nour local classical model: the model we want to construct is a linear\ncombination of the functions $A(\\boldsymbol{\\theta})$,\n$B_k(\\boldsymbol{\\theta})$ and $C_k(\\boldsymbol{\\theta})$ for each\nparameter, and $D_{kl}(\\boldsymbol{\\theta})$ for every pair of different\nparameters $(\\theta_k,\\theta_l)$.\n\nComputing the expansion coefficients\n------------------------------------\n\nWe can now use the derivatives of the function we are approximating to\nobtain the coefficients of the linear combination. As the terms we\ninclude in the expansion have leading orders $0$, $1$ and $2$, these\nderivatives are $E(\\boldsymbol{\\theta})$,\n$\\partial E(\\boldsymbol{\\theta})/\\partial \\theta_k$,\n$\\partial^2 E(\\boldsymbol{\\theta})/\\partial\\theta_k^2$, and\n$\\partial^2 E(\\boldsymbol{\\theta})/\\partial \\theta_k\\partial\\theta_l$.\nHowever, the trigonometric polynomials may contain multiple orders in\n$\\boldsymbol{\\theta}$. For example, both $A(\\boldsymbol{\\theta})$ and\n$C_k(\\boldsymbol{\\theta})$ contribute to the second order, so we have to\naccount for this in the coefficient of\n$\\partial^2 E(\\boldsymbol{\\theta})/\\partial \\theta_k^2$. We can name the\ndifferent coefficients (including the function value itself) accordingly\nto how we named the terms in the series:\n\n$$\\begin{aligned}\nE^{(A)} &= E(\\boldsymbol{\\theta})\\Bigg|_{\\boldsymbol{\\theta}=0} \\\\\nE^{(B)}_k &= \\frac{\\partial E(\\boldsymbol{\\theta})}{\\partial\\theta_k}\\Bigg|_{\\boldsymbol{\\theta}=0} \\\\\nE^{(C)}_k &= \\frac{\\partial^2 E(\\boldsymbol{\\theta})}{\\partial\\theta_k^2}\\Bigg|_{\\boldsymbol{\\theta}=0} + \\frac{1}{2}E(\\boldsymbol{\\theta})\\Bigg|_{\\boldsymbol{\\theta}=0}\\\\\nE^{(D)}_{kl} &= \\frac{\\partial^2 E(\\boldsymbol{\\theta})}{\\partial\\theta_k\\partial\\theta_l}\\Bigg|_{\\boldsymbol{\\theta}=0}\n\\end{aligned}$$\n\nIn PennyLane, computing the gradient of a cost function with respect to\nan array of parameters can be easily done with the [parameter-shift\nrule](https://pennylane.ai/qml/glossary/parameter_shift.html). By\niterating the rule, we can obtain the second derivatives \\-- the Hessian\n(see for example). Let us implement a function that does just that and\nprepares the coefficients $E^{(A/B/C/D)}$:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def get_model_data(fun, params):\n    \"\"\"Computes the coefficients for the classical model, E^(A), E^(B), E^(C), and E^(D).\"\"\"\n    num_params = len(params)\n\n    # E_A contains the energy at the reference point\n    E_A = fun(params)\n\n    # E_B contains the gradient.\n    E_B = qml.grad(fun)(params)\n\n    hessian = qml.jacobian(qml.grad(fun))(params)\n\n    # E_C contains the slightly adapted diagonal of the Hessian.\n    E_C = np.diag(hessian) + E_A / 2\n\n    # E_D contains the off-diagonal parts of the Hessian.\n    # We store each pair (k, l) only once, namely the upper triangle.\n    E_D = np.triu(hessian, 1)\n\n    return E_A, E_B, E_C, E_D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\\'s test our brand-new function for the circuit from above, at a\nrandom parameter position:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "parameters = np.random.random(2, requires_grad=True) * 2 * np.pi\nprint(f\"Random parameters (params): {parameters}\")\ncoeffs = get_model_data(circuit, parameters)\nprint(\n    f\"Coefficients at params:\",\n    f\" E_A = {coeffs[0]}\",\n    f\" E_B = {coeffs[1]}\",\n    f\" E_C = {coeffs[2]}\",\n    f\" E_D = {coeffs[3]}\",\n    sep=\"\\n\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The classical model (finally!)\n==============================\n\nBringing all of the above ingredients together, we have the following\ngorgeous trigonometric expansion up to second order:\n\n$$\\hat{E}(\\boldsymbol{\\theta}) := A(\\boldsymbol{\\theta}) E^{(A)} + \\sum_{k=1}^m\\left[B_k(\\boldsymbol{\\theta})E_k^{(B)} + C_k(\\boldsymbol{\\theta}) E_k^{(C)}\\right] + \\sum_{k<l}^m\\left[D_{kl}(\\boldsymbol{\\theta}) E_{kl}^{(D)}\\right].$$\n\nLet us now take a few moments to breath deeply and admire the entirety\nof it. On the one hand, we have the $A$, $B_k$, $C_k$, and $D_{kl}$\nfunctions, which we said are the basis functions of the expansion. On\nthe other hand we have the real-valued coefficients $E^{(A/B/C/D)}$ for\nthe previous functions which are nothing but the derivatives in the\ncorresponding input components. Combining them yields the trigonometric\nexpansion, which we implement with another function:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def model_cost(params, E_A, E_B, E_C, E_D):\n    \"\"\"Compute the model cost for relative parameters and given model data.\"\"\"\n    A = np.prod(np.cos(0.5 * params) ** 2)\n\n    # For the other terms we only compute the prefactor relative to A\n    B_over_A = 2 * np.tan(0.5 * params)\n    C_over_A = B_over_A ** 2 / 2\n    D_over_A = np.outer(B_over_A, B_over_A)\n\n    all_terms_over_A = [\n        E_A,\n        np.dot(E_B, B_over_A),\n        np.dot(E_C, C_over_A),\n        np.dot(B_over_A, E_D @ B_over_A),\n    ]\n\n    cost = A * np.sum(all_terms_over_A)\n\n    return cost\n\n\n# Compute the circuit at parameters (This value is also stored in E_A=coeffs[0])\nE_original = circuit(parameters)\n# Compute the model at parameters by plugging in relative parameters 0.\nE_model = model_cost(np.zeros_like(parameters), *coeffs)\nprint(\n    f\"The cost function at parameters:\",\n    f\"  Model:    {E_model}\",\n    f\"  Original: {E_original}\",\n    sep=\"\\n\",\n)\n# Check that coeffs[0] indeed is the original energy and that the model is correct at 0.\nprint(f\"E_A and E_original are the same: {coeffs[0]==E_original}\")\nprint(f\"E_model and E_original are the same: {E_model==E_original}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, the model reproduces the correct energy at the parameter\nposition $\\boldsymbol{\\theta}_0$ at which we constructed it (again note\nhow the input parameters of the model are *relative* to the reference\npoint such that $\\hat{E}(0)=E(\\boldsymbol{\\theta}_0)$ is satisfied).\nWhen we move away from $\\boldsymbol{\\theta}_0$, the model starts to\ndeviate, as it is an *approximation* after all:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Obtain a random shift away from parameters\nshift = 0.1 * np.random.random(2)\nprint(f\"Shift parameters by the vector {np.round(shift, 4)}.\")\nnew_parameters = parameters + shift\n# Compute the cost function and the model at the shifted position.\nE_original = circuit(new_parameters)\nE_model = model_cost(shift, *coeffs)\nprint(\n    f\"The cost function at parameters:\",\n    f\"  Model:    {E_model}\",\n    f\"  Original: {E_original}\",\n    sep=\"\\n\",\n)\nprint(f\"E_model and E_original are the same: {E_model==E_original}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.note}\n::: {.title}\nNote\n:::\n\n**Counting parameters and evaluations**\n\nHow many parameters does our model have? In the following table we count\nthem for an $m$-dimensional input variable\n$\\boldsymbol{\\theta}=(\\theta_1,\\ldots,\\theta_m)$:\n\n  -------------------------------------------------------------------------------\n                 Number of coefficients           Number of circuit evaluations\n  -------------- -------------------------------- -------------------------------\n  $E^{(A)}$      $1$                              $1$\n\n  $E^{(B)}$      $m$                              $2m$\n\n  $E^{(C)}$      $m$                              $m$\n\n  $E^{(D)}$      $\\frac{m(m-1)}{2}$               $4\\frac{m(m-1)}{2}$\n\n  Total:         $\\frac{m^2}{2}+\\frac{3m}{2}+1$   $2m^2+m+1$\n  -------------------------------------------------------------------------------\n\nSo there we go! We only need polynomially many parameters and circuit\nevaluations. This is much cheaper than the $3^m$ we would need if we\nnaively tried to construct the cost landscape exactly, without chopping\nafter second order.\n:::\n\nNow this should be enough theory, so let\\'s visualize the model that\nresults from our trigonometric expansion. We\\'ll use the coefficients\nand the `model_cost` function from above and sample a new random\nparameter position.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\nfrom itertools import product\n\n# We actually make the plotting a function because we will reuse it below.\ndef plot_cost_and_model(f, model, params, shift_radius=5 * np.pi / 8, num_points=20):\n    \"\"\"Plot a function and a model of the function as well as its deviation.\"\"\"\n\n    coords = np.linspace(-shift_radius, shift_radius, num_points)\n    X, Y = np.meshgrid(coords + params[0], coords + params[1])\n\n    # Compute the original cost function and the model on the grid.\n    Z_original = np.array([[f(params + np.array([t1, t2])) for t2 in coords] for t1 in coords])\n    Z_model = np.array([[model(np.array([t1, t2])) for t2 in coords] for t1 in coords])\n\n    # Prepare sampled points for plotting rods.\n    shifts = [-np.pi / 2, 0, np.pi / 2]\n    samples = []\n    for s1, s2 in product(shifts, repeat=2):\n        shifted_params = params + np.array([s1, s2])\n        samples.append([*(params+np.array([s2, s1])), f(shifted_params)])\n\n    # Display landscapes incl. sampled points and deviation.\n    alpha = 0.6\n    fig, (ax0, ax1, ax2) = plt.subplots(1, 3, subplot_kw={\"projection\": \"3d\"}, figsize=(10, 4))\n    green = \"#209494\"\n    orange = \"#ED7D31\"\n    red = \"xkcd:brick red\"\n    surf = ax0.plot_surface(X, Y, Z_original, color=green, alpha=alpha)\n    ax0.set_title(\"Original energy and samples\")\n    ax1.plot_surface(X, Y, Z_model, color=orange, alpha=alpha)\n    ax1.set_title(\"Model energy\")\n    ax2.plot_surface(X, Y, Z_original - Z_model, color=red, alpha=alpha)\n    ax2.set_title(\"Deviation\")\n    for s in samples:\n        ax0.plot([s[0]] * 2, [s[1]] * 2, [np.min(Z_original) - 0.2, s[2]], color=\"k\")\n    for ax, z in zip((ax0, ax1), (f(params), model(0 * params))):\n        ax.plot([params[0]] * 2, [params[1]] * 2, [np.min(Z_original) - 0.2, z], color=\"k\")\n        ax.scatter([params[0]], [params[1]], [z], color=\"k\", marker=\"o\")\n    plt.tight_layout(pad=2, w_pad=2.5)\n\n\n# Get some fresh random parameters and the model coefficients\nparameters = np.random.random(2, requires_grad=True) * 2 * np.pi\ncoeffs = get_model_data(circuit, parameters)\n\n# Define a mapped model that has the model coefficients fixed.\nmapped_model = lambda params: model_cost(params, *coeffs)\nplot_cost_and_model(circuit, mapped_model, parameters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the first two plots, we see the true landscape, and the approximate\nmodel. The vertical rods indicate the points at which the original cost\nfunction was evaluated in order to obtain the model coefficients (we\nskip the additional evaluations for $E^{(C)}$, though, for clarity of\nthe plot). The rod with the bead on top indicates the reference point\naround which the model is built and at which it coincides with the\noriginal cost function up to second order. This is underlined in the\nthird plot, where we see the difference between the model and true\nlandscapes. Around the reference point the difference is very small and\nchanges very slowly, only growing significantly for large simultaneous\nperturbations in both parameters. This already hints at the value of the\nmodel for local optimization.\n\nQuantum Analytic Descent\n========================\n\nThe underlying idea we will now try to exploit for optimization in VQEs\nis the following: if we can model the cost around the reference point\nwell enough, we will be able to find a rough estimate of where the\nminimum of the landscape is. Granted, our model represents the true\nlandscape less accurately the further we go away from the reference\npoint $\\boldsymbol{\\theta}_0$, but nonetheless the minimum *of the\nmodel* will bring us much closer to the minimum *of the true cost* than\na random choice. Recall the complete strategy from above:\n\n1.  Set an initial reference point $\\boldsymbol{\\theta}_0$.\n2.  Build the model\n    $\\hat{E}(\\boldsymbol{\\theta})\\approx E(\\boldsymbol{\\theta}_0+\\boldsymbol{\\theta})$\n    at this point.\n3.  Find the minimum $\\boldsymbol{\\theta}_\\text{min}$ of the model.\n4.  Set $\\boldsymbol{\\theta}_0+\\boldsymbol{\\theta}_\\text{min}$ as the\n    new reference point $\\boldsymbol{\\theta}_0$, go back to Step 2.\n5.  After convergence or a fixed number of models built, output the last\n    minimum\n    $\\boldsymbol{\\theta}_\\text{opt}=\\boldsymbol{\\theta}_0+\\boldsymbol{\\theta}_\\text{min}$.\n\nThis provides an iterative strategy which will take us to a good enough\nsolution in fewer iterations than, for example, regular stochastic\ngradient descent (SGD). The procedure of Quantum Analytic Descent is\nalso shown in the following flowchart. Note that the minimization of the\nmodel in Step 3 is carried out via an inner optimization loop.\n\n![](../demonstrations/quantum_analytic_descent/flowchart.png){.align-center\nwidth=\"80.0%\"}\n\nUsing the functions from above, we now can implement the loop between\nSteps 2 and 4. Indeed, for a relatively small number of iterations we\nshould already find a low enough value. If we look back at the circuit\nwe defined, we notice that we are measuring the observable\n\nwhich has the eigenvalues $1$ and $-1$. This means our function is\nbounded and takes values in the range $[-1,1]$, so that the global\nminimum should be around $-1$ if our circuit is expressive enough.\nLet\\'s try it and apply the full optimization strategy:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import copy\n\n# Set the number of iterations of Steps 2, 3, and 4\nN_iter_outer = 3\nN_iter_inner = 50\n\npast_coeffs = []\npast_parameters = []\ncircuit_log = [circuit(parameters)]\nmodel_logs = []\n\nfor iter_outer in range(N_iter_outer):\n    # Model building phase of outer iteration - step 2.\n    coeffs = get_model_data(circuit, parameters)\n    past_coeffs.append(copy.deepcopy(coeffs))\n    past_parameters.append(parameters.copy())\n    # Map the model to be only depending on the parameters, not the coefficients.\n    mapped_model = lambda params: model_cost(params, *coeffs)\n\n    if iter_outer == 0:\n        print(f\"True energy at initial parameters: {np.round(coeffs[0], decimals=4)}\\n\")\n\n    opt = qml.AdamOptimizer(0.05)\n    # Recall that the parameters of the model are relative coordinates.\n    # Correspondingly, we initialize at 0, not at parameters.\n    relative_parameters = np.zeros_like(parameters, requires_grad=True)\n    model_log = [mapped_model(relative_parameters)]\n    print(f\"-Iteration {iter_outer+1}-\")\n\n    # Run the optimizer for N_iter_inner epochs - Step 3.\n    for iter_inner in range(N_iter_inner):\n        relative_parameters = opt.step(mapped_model, relative_parameters)\n        circuit_log.append(circuit(parameters + relative_parameters))\n        model_log.append(mapped_model(relative_parameters))\n        if (iter_inner + 1) % 50 == 0:\n            E_model = mapped_model(relative_parameters)\n            print(\n                f\"Epoch {iter_inner+1:4d}: Model cost = {np.round(E_model, 4)}\",\n                f\" at relative parameters {np.round(relative_parameters, 4)}\",\n            )\n\n    # Store the relative parameters that minimize the model by adding the shift - Step 4.\n    parameters += relative_parameters\n    E_original = circuit(parameters)\n    print(f\"True energy at the minimum of the model: {E_original}\")\n    print(f\"New reference parameters: {np.round(parameters, 4)}\\n\")\n    model_logs.append(model_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This looks great! Quantum Analytic Descent found the minimum.\n\nInspecting the models\n=====================\n\nLet us take a look at the intermediate models QAD built:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mapped_model = lambda params: model_cost(params, *past_coeffs[0])\nplot_cost_and_model(circuit, mapped_model, past_parameters[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Iteration 1:** We see the cost function and the model around our\nstarting point. This is the same as the plot before.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mapped_model = lambda params: model_cost(params, *past_coeffs[1])\nplot_cost_and_model(circuit, mapped_model, past_parameters[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Iteration 2:** Now we observe the model better resembles the original\nlandscape. In addition, the minimum of the model is within the displayed\nrange \\-- we\\'re getting closer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mapped_model = lambda params: model_cost(params, *past_coeffs[2])\nplot_cost_and_model(circuit, mapped_model, past_parameters[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Iteration 3:** Both the model and the original cost function now show\na minimum close to our parameter position\\-\\-- Quantum Analytic Descent\nconverged. Note how the larger deviations of the model close to the\nboundaries are not a problem at all because we only use the model in the\ncentral area in which both the original energy and the model form a\nconvex bowl and the deviation plateaus at zero.\n\nOptimization behaviour\n======================\n\nIf we pay close attention to the values printed during the optimization,\nwe can identify a curious phenomenon. At the last epochs within some\niterations, the *model cost* goes beyond $-1$. Could we visualize this\nbehavior more clearly, please?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(6, 4))\nax.plot(circuit_log, color=\"#209494\", label=\"True\")\nfor i in range(N_iter_outer):\n    x = range(i * N_iter_inner, (i + 1) * N_iter_inner + 1)\n    (line,) = ax.plot(x, model_logs[i], ls=\"--\", color=\"#ED7D31\")\n    if i == 0:\n        line.set_label(\"Model\")\nax.plot([0, N_iter_outer * N_iter_inner], [-1.0, -1.0], lw=0.6, color=\"0.6\", label=\"Solution\")\nax.set_xlabel(\"epochs\")\nax.set_ylabel(\"cost\")\nleg = ax.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each of the orange lines corresponds to minimizing the model constructed\nat a different reference point. We can now more easily appreciate the\nphenomenon we just described: towards the end of each \\\"outer\\\"\noptimization step, the model cost can potentially be significantly lower\nthan the true cost. Once the true cost itself approaches the absolute\nminimum, this means the model cost can overstep the allowed range.\n*Wasn\\'t this forbidden? You guys told us the function could only take\nvalues in* $[-1,1]$ \\>:@ Yes, but careful! While the *true cost* values\nare bounded, that does not mean the ones of the *model* are! Notice also\nhow this only happens at the first stages of analytic descent.\n\nBringing together a few chords we have touched so far: once we fix a\nreference value, the further we go from it, the less accurate our model\nbecomes. Thus, if we start far off from the true minimum, it can happen\nthat our model exaggerates how steep the landscape is and then the model\nminimum lies lower than that of the true cost. We see how values exiting\nthe allowed range of the true cost function does not have an impact on\nthe overall optimization.\n\nIn this demo we\\'ve seen how to implement the Quantum Analytic Descent\nalgorithm using PennyLane for a toy example of a Variational Quantum\nEigensolver. By making extensive use of 3D plots we have also tried to\nillustrate exactly what is going on in both the true cost landscape and\nthe trigonometric expansion approximation. Recall we wanted to avoid\nworking on the true landscape itself because we can only access it via\nvery costly quantum computations. Instead, a fixed number of runs on the\nquantum device for a few iterations allowed us to construct a classical\nmodel on which we performed (cheap) classical optimization.\n\nAnd that was it! Thanks for coming to our show. Don\\'t forget to fasten\nyour seat belts on your way home! It was a pleasure having you here\ntoday.\n\nReferences\n==========\n\nAbout the authors\n=================\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}