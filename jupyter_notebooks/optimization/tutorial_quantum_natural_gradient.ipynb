{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# This cell is added by sphinx-gallery\n# It can be customized to whatever you like\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quantum natural gradient {#quantum_natural_gradient}\n========================\n\n::: {.meta}\n:property=\\\"og:description\\\": The quantum natural gradient method can\nachieve faster convergence for quantum machine learning problems by\ntaking into account the intrinsic geometry of qubits.\n:property=\\\"og:image\\\":\n<https://pennylane.ai/qml/_images/qng_optimization.png>\n:::\n\n::: {.related}\ntutorial\\_backprop Quantum gradients with backpropagation\ntutorial\\_vqe\\_qng Accelerating VQE with quantum natural gradient\n:::\n\n*Author: Josh Izaac --- Posted: 11 October 2019. Last updated: 25\nJanuary 2021.*\n\nThis example demonstrates the quantum natural gradient optimization\ntechnique for variational quantum circuits, originally proposed in\n[Stokes et al. (2019)](https://arxiv.org/abs/1909.02108).\n\nBackground\n----------\n\nThe most successful class of quantum algorithms for use on near-term\nnoisy quantum hardware is the so-called variational quantum algorithm.\nAs laid out in the\n`Concepts section <glossary_variational_circuit>`{.interpreted-text\nrole=\"ref\"}, in variational quantum algorithms a low-depth parametrized\nquantum circuit ansatz is chosen, and a problem-specific observable\nmeasured. A classical optimization loop is then used to find the set of\nquantum parameters that *minimize* a particular measurement expectation\nvalue of the quantum device. Examples of such algorithms include the\n`variational quantum\neigensolver (VQE) <tutorial_vqe>`{.interpreted-text role=\"doc\"}, the\n[quantum approximate optimization algorithm\n(QAOA)](https://arxiv.org/abs/1411.4028), and\n`quantum neural networks (QNN) <quantum_neural_net>`{.interpreted-text\nrole=\"ref\"}.\n\nMost recent demonstrations of variational quantum algorithms have used\ngradient-free classical optimization methods, such as the Nelder-Mead\nalgorithm. However, the parameter-shift rule (as implemented in\nPennyLane) allows the user to automatically compute analytic gradients\nof quantum circuits. This opens up the possibility to train quantum\ncomputing hardware using gradient descent\\-\\--the same method used to\ntrain deep learning models. Though one caveat has surfaced with gradient\ndescent \\-\\-- how do we choose the optimal step size for our variational\nquantum algorithms, to ensure successful and efficient optimization?\n\n### The natural gradient\n\nIn standard gradient descent, each optimization step is given by\n\n$$\\theta_{t+1} = \\theta_t -\\eta \\nabla \\mathcal{L}(\\theta),$$\n\nwhere $\\mathcal{L}(\\theta)$ is the cost as a function of the parameters\n$\\theta$, and $\\eta$ is the learning rate or step size. In essence, each\noptimization step calculates the steepest descent direction around the\nlocal value of $\\theta_t$ in the parameter space, and updates\n$\\theta_t\\rightarrow \\theta_{t+1}$ by this vector.\n\nThe problem with the above approach is that each optimization step is\nstrongly connected to a *Euclidean geometry* on the parameter space. The\nparametrization is not unique, and different parametrizations can\ndistort distances within the optimization landscape.\n\nFor example, consider the following cost function $\\mathcal{L}$,\nparametrized using two different coordinate systems,\n$(\\theta_0, \\theta_1)$, and $(\\phi_0, \\phi_1)$:\n\n| \n\n![](../demonstrations/quantum_natural_gradient/qng7.png){.align-center\nwidth=\"90.0%\"}\n\n| \n\nPerforming gradient descent in the $(\\theta_0, \\theta_1)$ parameter\nspace, we are updating each parameter by the same Euclidean distance,\nand not taking into account the fact that the cost function might vary\nat a different rate with respect to each parameter.\n\nInstead, if we perform a change of coordinate system\n(re-parametrization) of the cost function, we might find a parameter\nspace where variations in $\\mathcal{L}$ are similar across different\nparameters. This is the case with the new parametrization\n$(\\phi_0, \\phi_1)$; the cost function is unchanged, but we now have a\nnicer geometry in which to perform gradient descent, and a more\ninformative stepsize. This leads to faster convergence, and can help\navoid optimization becoming stuck in local minima. For a more in-depth\nexplanation, including why the parameter space might not be best\nrepresented by a Euclidean space, see [Yamamoto\n(2019)](https://arxiv.org/abs/1909.05074).\n\nHowever, what if we avoid gradient descent in the parameter space\naltogether? If we instead consider the optimization problem as a\nprobability distribution of possible output values given an input (i.e.,\n[maximum likelihood\nestimation](https://en.wikipedia.org/wiki/Likelihood_function)), a\nbetter approach is to perform the gradient descent in the *distribution\nspace*, which is dimensionless and invariant with respect to the\nparametrization. As a result, each optimization step will always choose\nthe optimum step-size for every parameter, regardless of the\nparametrization.\n\nIn classical neural networks, the above process is known as *natural\ngradient descent*, and was first introduced by [Amari\n(1998)](https://www.mitpressjournals.org/doi/abs/10.1162/089976698300017746).\nThe standard gradient descent is modified as follows:\n\n$$\\theta_{t+1} = \\theta_t - \\eta F^{-1}\\nabla \\mathcal{L}(\\theta),$$\n\nwhere $F$ is the [Fisher information\nmatrix](https://en.wikipedia.org/wiki/Fisher_information#Matrix_form).\nThe Fisher information matrix acts as a metric tensor, transforming the\nsteepest descent in the Euclidean parameter space to the steepest\ndescent in the distribution space.\n\n### The quantum analog\n\nIn a similar vein, it has been shown that the standard Euclidean\ngeometry is sub-optimal for optimization of quantum variational\nalgorithms [(Harrow and Napp, 2019)](https://arxiv.org/abs/1901.05374).\nThe space of quantum states instead possesses a unique invariant metric\ntensor known as the Fubini-Study metric tensor $g_{ij}$, which can be\nused to construct a quantum analog to natural gradient descent:\n\n$$\\theta_{t+1} = \\theta_t - \\eta g^{+}(\\theta_t)\\nabla \\mathcal{L}(\\theta),$$\n\nwhere $g^{+}$ refers to the pseudo-inverse.\n\n::: {.note}\n::: {.title}\nNote\n:::\n\nIt can be shown that the Fubini-Study metric tensor reduces to the\nFisher information matrix in the classical limit.\n\nFurthermore, in the limit where $\\eta\\rightarrow 0$, the dynamics of the\nsystem are equivalent to imaginary-time evolution within the variational\nsubspace, as proposed in [McArdle et al.\n(2018)](https://arxiv.org/abs/1804.03023).\n:::\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Block-diagonal metric tensor\n============================\n\nA block-diagonal approximation to the Fubini-Study metric tensor of a\nvariational quantum circuit can be evaluated on quantum hardware.\n\nConsider a variational quantum circuit\n\n$$U(\\mathbf{\\theta})|\\psi_0\\rangle = V_L(\\theta_L) W_L V_{L-1}(\\theta_{L-1}) W_{L-1}\n  \\cdots V_{\\ell}(\\theta_{\\ell}) W_{\\ell} \\cdots V_{0}(\\theta_{0}) W_{0} |\\psi_0\\rangle$$\n\nwhere\n\n-   $|\\psi_0\\rangle$ is the initial state,\n-   $W_\\ell$ are layers of non-parametrized quantum gates,\n-   $V_\\ell(\\theta_\\ell)$ are layers of parametrized quantum gates with\n    $n_\\ell$ parameters\n    $\\theta_\\ell = \\{\\theta^{(\\ell)}_0, \\dots, \\theta^{(\\ell)}_n\\}$.\n\nFurther, assume all parametrized gates can be written in the form\n$X(\\theta^{(\\ell)}_{i}) = e^{i\\theta^{(\\ell)}_{i} K^{(\\ell)}_i}$, where\n$K^{(\\ell)}_i$ is the *generator* of the parametrized operation.\n\nFor each parametric layer $\\ell$ in the variational quantum circuit the\n$n_\\ell\\times n_\\ell$ block-diagonal submatrix of the Fubini-Study\ntensor $g_{ij}^{(\\ell)}$ is calculated by:\n\n$$g_{ij}^{(\\ell)} = \\langle \\psi_{\\ell-1} | K_i K_j | \\psi_{\\ell-1} \\rangle\n- \\langle \\psi_{\\ell-1} | K_i | \\psi_{\\ell-1}\\rangle\n\\langle \\psi_{\\ell-1} |K_j | \\psi_{\\ell-1}\\rangle$$\n\nwhere\n\n$$| \\psi_{\\ell-1}\\rangle = V_{\\ell-1}(\\theta_{\\ell-1}) W_{\\ell-1} \\cdots V_{0}(\\theta_{0}) W_{0} |\\psi_0\\rangle.$$\n\n(that is, $|\\psi_{\\ell-1}\\rangle$ is the quantum state prior to the\napplication of parameterized layer $\\ell$), and we have\n$K_i \\equiv K_i^{(\\ell)}$ for brevity.\n\nLet\\'s consider a small variational quantum circuit example coded in\nPennyLane:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pennylane as qml\nfrom pennylane import numpy as np\n\ndev = qml.device(\"default.qubit\", wires=3)\n\n\n@qml.qnode(dev, interface=\"autograd\")\ndef circuit(params):\n    # |psi_0>: state preparation\n    qml.RY(np.pi / 4, wires=0)\n    qml.RY(np.pi / 3, wires=1)\n    qml.RY(np.pi / 7, wires=2)\n\n    # V0(theta0, theta1): Parametrized layer 0\n    qml.RZ(params[0], wires=0)\n    qml.RZ(params[1], wires=1)\n\n    # W1: non-parametrized gates\n    qml.CNOT(wires=[0, 1])\n    qml.CNOT(wires=[1, 2])\n\n    # V_1(theta2, theta3): Parametrized layer 1\n    qml.RY(params[2], wires=1)\n    qml.RX(params[3], wires=2)\n\n    # W2: non-parametrized gates\n    qml.CNOT(wires=[0, 1])\n    qml.CNOT(wires=[1, 2])\n\n    return qml.expval(qml.PauliY(0))\n\n\nparams = np.array([0.432, -0.123, 0.543, 0.233])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The above circuit consists of 4 parameters, with two distinct\nparametrized layers of 2 parameters each.\n\n![](../demonstrations/quantum_natural_gradient/qng1.png){.align-center\nwidth=\"90.0%\"}\n\n| \n\n(Note that in this example, the first non-parametrized layer $W_0$ is\nsimply the identity.) Since there are two layers, each with two\nparameters, the block-diagonal approximation consists of two $2\\times 2$\nmatrices, $g^{(0)}$ and $g^{(1)}$.\n\n![](../demonstrations/quantum_natural_gradient/qng2.png){.align-center\nwidth=\"30.0%\"}\n\nTo compute the first block-diagonal $g^{(0)}$, we create subcircuits\nconsisting of all gates prior to the layer, and observables\ncorresponding to the *generators* of the gates in the layer:\n\n![](../demonstrations/quantum_natural_gradient/qng3.png){.align-center\nwidth=\"30.0%\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "g0 = np.zeros([2, 2])\n\n\ndef layer0_subcircuit(params):\n    \"\"\"This function contains all gates that\n    precede parametrized layer 0\"\"\"\n    qml.RY(np.pi / 4, wires=0)\n    qml.RY(np.pi / 3, wires=1)\n    qml.RY(np.pi / 7, wires=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then post-process the measurement results in order to determine\n$g^{(0)}$, as follows.\n\n![](../demonstrations/quantum_natural_gradient/qng4.png){.align-center\nwidth=\"50.0%\"}\n\nWe can see that the diagonal terms are simply given by the variance:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@qml.qnode(dev, interface=\"autograd\")\ndef layer0_diag(params):\n    layer0_subcircuit(params)\n    return qml.var(qml.PauliZ(0)), qml.var(qml.PauliZ(1))\n\n\n# calculate the diagonal terms\nvarK0, varK1 = layer0_diag(params)\ng0[0, 0] = varK0 / 4\ng0[1, 1] = varK1 / 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following two subcircuits are then used to calculate the\noff-diagonal covariance terms of $g^{(0)}$:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@qml.qnode(dev, interface=\"autograd\")\ndef layer0_off_diag_single(params):\n    layer0_subcircuit(params)\n    return qml.expval(qml.PauliZ(0)), qml.expval(qml.PauliZ(1))\n\n\n@qml.qnode(dev, interface=\"autograd\")\ndef layer0_off_diag_double(params):\n    layer0_subcircuit(params)\n    ZZ = np.kron(np.diag([1, -1]), np.diag([1, -1]))\n    return qml.expval(qml.Hermitian(ZZ, wires=[0, 1]))\n\n\n# calculate the off-diagonal terms\nexK0, exK1 = layer0_off_diag_single(params)\nexK0K1 = layer0_off_diag_double(params)\n\ng0[0, 1] = (exK0K1 - exK0 * exK1) / 4\ng0[1, 0] = (exK0K1 - exK0 * exK1) / 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that, by definition, the block-diagonal matrices must be real and\nsymmetric.\n\nWe can repeat the above process to compute $g^{(1)}$. The subcircuit\nrequired is given by\n\n![](../demonstrations/quantum_natural_gradient/qng8.png){.align-center\nwidth=\"50.0%\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "g1 = np.zeros([2, 2])\n\n\ndef layer1_subcircuit(params):\n    \"\"\"This function contains all gates that\n    precede parametrized layer 1\"\"\"\n    # |psi_0>: state preparation\n    qml.RY(np.pi / 4, wires=0)\n    qml.RY(np.pi / 3, wires=1)\n    qml.RY(np.pi / 7, wires=2)\n\n    # V0(theta0, theta1): Parametrized layer 0\n    qml.RZ(params[0], wires=0)\n    qml.RZ(params[1], wires=1)\n\n    # W1: non-parametrized gates\n    qml.CNOT(wires=[0, 1])\n    qml.CNOT(wires=[1, 2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using this subcircuit, we can now generate the submatrix $g^{(1)}$.\n\n![](../demonstrations/quantum_natural_gradient/qng5.png){.align-center\nwidth=\"50.0%\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@qml.qnode(dev, interface=\"autograd\")\ndef layer1_diag(params):\n    layer1_subcircuit(params)\n    return qml.var(qml.PauliY(1)), qml.var(qml.PauliX(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As previously, the diagonal terms are simply given by the variance,\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "varK0, varK1 = layer1_diag(params)\ng1[0, 0] = varK0 / 4\ng1[1, 1] = varK1 / 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "while the off-diagonal terms require covariance between the two\nobservables to be computed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@qml.qnode(dev, interface=\"autograd\")\ndef layer1_off_diag_single(params):\n    layer1_subcircuit(params)\n    return qml.expval(qml.PauliY(1)), qml.expval(qml.PauliX(2))\n\n\n@qml.qnode(dev, interface=\"autograd\")\ndef layer1_off_diag_double(params):\n    layer1_subcircuit(params)\n    X = np.array([[0, 1], [1, 0]])\n    Y = np.array([[0, -1j], [1j, 0]])\n    YX = np.kron(Y, X)\n    return qml.expval(qml.Hermitian(YX, wires=[1, 2]))\n\n\n# calculate the off-diagonal terms\nexK0, exK1 = layer1_off_diag_single(params)\nexK0K1 = layer1_off_diag_double(params)\n\ng1[0, 1] = (exK0K1 - exK0 * exK1) / 4\ng1[1, 0] = g1[0, 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Putting this altogether, the block-diagonal approximation to the\nFubini-Study metric tensor for this variational quantum circuit is\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from scipy.linalg import block_diag\n\ng = block_diag(g0, g1)\nprint(np.round(g, 8))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PennyLane contains a built-in function for computing the Fubini-Study\nmetric tensor, `~.pennylane.metric_tensor`{.interpreted-text\nrole=\"func\"}, which we can use to verify this result:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(np.round(qml.metric_tensor(circuit, approx=\"block-diag\")(params), 8))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As opposed to our manual computation, which required 6 different quantum\nevaluations, the PennyLane Fubini-Study metric tensor implementation\nrequires only 2 quantum evaluations, one per layer. This is done by\nautomatically detecting the layer structure, and noting that every\nobservable that must be measured commutes, allowing for simultaneous\nmeasurement.\n\nTherefore, by combining the quantum natural gradient optimizer with the\nanalytic parameter-shift rule to optimize a variational circuit with $d$\nparameters and $L$ parametrized layers, a total of $2d+L$ quantum\nevaluations are required per optimization step.\n\nNote that the `~.pennylane.metric_tensor`{.interpreted-text role=\"func\"}\nfunction also supports computing the diagonal approximation to the\nmetric tensor:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(qml.metric_tensor(circuit, approx='diag')(params))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Furthermore, the returned metric tensor is **full differentiable**;\ninclude it in your cost function, and train or optimize its value!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quantum natural gradient optimization\n=====================================\n\nPennyLane provides an implementation of the quantum natural gradient\noptimizer, `~.pennylane.QNGOptimizer`{.interpreted-text role=\"class\"}.\nLet\\'s compare the optimization convergence of the QNG Optimizer and the\n`~.pennylane.GradientDescentOptimizer`{.interpreted-text role=\"class\"}\nfor the simple variational circuit above.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "steps = 200\ninit_params = np.array([0.432, -0.123, 0.543, 0.233], requires_grad=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Performing vanilla gradient descent:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "gd_cost = []\nopt = qml.GradientDescentOptimizer(0.01)\n\ntheta = init_params\nfor _ in range(steps):\n    theta = opt.step(circuit, theta)\n    gd_cost.append(circuit(theta))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Performing quantum natural gradient descent:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "qng_cost = []\nopt = qml.QNGOptimizer(0.01)\n\ntheta = init_params\nfor _ in range(steps):\n    theta = opt.step(circuit, theta)\n    qng_cost.append(circuit(theta))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plotting the cost vs optimization step for both optimization strategies:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n\nplt.style.use(\"seaborn\")\nplt.plot(gd_cost, \"b\", label=\"Vanilla gradient descent\")\nplt.plot(qng_cost, \"g\", label=\"Quantum natural gradient descent\")\n\nplt.ylabel(\"Cost function value\")\nplt.xlabel(\"Optimization steps\")\nplt.legend()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "References\n==========\n\n1.  Shun-Ichi Amari. \\\"Natural gradient works efficiently in learning.\\\"\n    [Neural computation 10.2,\n    251-276](https://www.mitpressjournals.org/doi/abs/10.1162/089976698300017746), 1998.\n2.  James Stokes, Josh Izaac, Nathan Killoran, Giuseppe Carleo.\n    \\\"Quantum Natural Gradient.\\\"\n    [arXiv:1909.02108](https://arxiv.org/abs/1909.02108), 2019.\n3.  Aram Harrow and John Napp. \\\"Low-depth gradient measurements can\n    improve convergence in variational hybrid quantum-classical\n    algorithms.\\\"\n    [arXiv:1901.05374](https://arxiv.org/abs/1901.05374), 2019.\n4.  Naoki Yamamoto. \\\"On the natural gradient for variational quantum\n    eigensolver.\\\"\n    [arXiv:1909.05074](https://arxiv.org/abs/1909.05074), 2019.\n\nAbout the author\n================\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}