{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# This cell is added by sphinx-gallery\n# It can be customized to whatever you like\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generalization in QML from few training data {#learning_few_data}\n============================================\n\n::: {.meta}\n:property=\\\"og:description\\\": Generalization of quantum machine learning\nmodels. :property=\\\"og:image\\\":\n<https://pennylane.ai/qml/_images/few_data_thumbnail.png>\n:::\n\n::: {.related}\ntutorial\\_local\\_cost\\_functions Alleviating barren plateaus with local\ncost functions\n:::\n\n*Authors: Korbinian Kottmann, Luis Mantilla Calderon, Maurice Weber ---\nPosted: 29 August 2022*\n\nIn this tutorial, we dive into the generalization capabilities of\nquantum machine learning models. For the example of a [Quantum\nConvolutional Neural Network\n(QCNN)](https://pennylane.ai/qml/glossary/qcnn.html), we show how its\ngeneralization error behaves as a function of the number of training\nsamples. This demo is based on the paper *\\\"Generalization in quantum\nmachine learning from few training data\\\"*. by Caro et al..\n\nWhat is generalization in (Q)ML?\n--------------------------------\n\nWhen optimizing a machine learning model, be it classical or quantum, we\naim to maximize its performance over the data distribution of interest\n(e.g., images of cats and dogs). However, in practice, we are limited to\na finite amount of data, which is why it is necessary to reason about\nhow our model performs on new, previously unseen data. The difference\nbetween the model\\'s performance on the true data distribution and the\nperformance estimated from our training data is called the\n*generalization error*, and it indicates how well the model has learned\nto generalize to unseen data. Generalization can be seen as a\nmanifestation of the bias-variance trade-off; models that perfectly fit\nthe training data admit a low bias at the cost of a higher variance, and\nhence typically perform poorly on unseen test data. In the classical\nmachine learning community, this trade-off has been extensively studied\nand has led to optimization techniques that favour generalization, for\nexample, by regularizing models via their variance. Below, we see a\ncanoncial example of this trade-off, with a model having low bias, but\nhigh variance and therefore high generalization error. The low variance\nmodel, on the other hand, has a higher bias but generalizes better.\n\n![](/demonstrations/learning_few_data/overfitting.png){.align-center\nwidth=\"65.0%\"}\n\nLet us now dive deeper into generalization properties of quantum machine\nlearning (QML) models. We start by describing the typical data\nprocessing pipeline of a QML model. A classical data input $x$ is first\nencoded in a quantum state via a mapping $x \\mapsto \\rho(x)$. This\nencoded state is then processed through a quantum channel\n$\\rho(x) \\mapsto \\mathcal{E}_\\alpha(\\rho(x))$ with learnable parameters\n$\\alpha$. Finally, a measurement is performed on the resulting state to\nget the final prediction. Now, the goal is to minimize the expected loss\nover the data-generating distribution $P$, indicating how well our model\nperforms on new data. Mathematically, for a loss function $\\ell$, the\nexpected loss, denoted by $R$, is given by\n\n$$R(\\alpha) = \\mathbb{E}_{(x,y)\\sim P}[\\ell(\\alpha;\\,x,\\,y)]$$\n\nwhere $x$ are the features, $y$ are the labels, and $P$ is their joint\ndistribution. In practice, as the joint distribution $P$ is generally\nunknown, this quantity has to be estimated from a finite amount of data.\nGiven a training set $S = \\{(x_i,\\,y_i)\\}_{i=1}^N$ with $N$ samples, we\nestimate the performance of our QML model by calculating the average\nloss over the training set\n\n$$R_S(\\alpha) = \\frac{1}{N}\\sum_{i=1}^N \\ell(\\alpha;\\,x_i,\\,y_i),$$\n\nwhich is referred to as the training loss and is an unbiased estimate of\n$R(\\alpha)$. This is only a proxy to the true quantity of interest\n$R(\\alpha)$, and their difference is called the generalization error\n\n$$\\mathrm{gen}(\\alpha) =  R(\\alpha) - \\hat{R}_S(\\alpha),$$\n\nwhich is the quantity that we explore in this tutorial. Keeping in mind\nthe bias-variance trade-off, one would expect that more complex models,\ni.e. models with a larger number of parameters, achieve a lower error on\nthe training data but a higher generalization error. Having more\ntraining data, on the other hand, leads to a better approximation of the\ntrue expected loss and hence a lower generalization error. This\nintuition is made precise in Ref., where it is shown that\n$\\mathrm{gen}(\\alpha)$ roughly scales as $\\mathcal{O}(\\sqrt{T / N})$,\nwhere $T$ is the number of parametrized gates and $N$ is the number of\ntraining samples.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generalization bounds for QML models\n====================================\n\nAs hinted at earlier, we expect the generalization error to depend both\non the richness of the model class, as well as on the amount of training\ndata available. As a first result, the authors of Ref. found that for a\nQML model with at most $T$ parametrized local quantum channels, the\ngeneralization error depends on $T$ and $N$ according to\n\n$$\\mathrm{gen}(\\alpha) \\sim \\mathcal{O}\\left(\\sqrt{\\frac{T\\log T}{N}}\\right).$$\n\nWe see that this scaling is in line with our intuition that the\ngeneralization error scales inversely with the number of training\nsamples and increases with the number of parametrized gates. However, as\nis the case for [quantum convolutional neural networks\n(QCNNs)](https://pennylane.ai/qml/glossary/qcnn.html), it is possible to\nget a more fine-grained bound by including knowledge on the number of\ngates $M$ which have been reused (i.e. whose parameters are shared\nacross wires). Naively, one could suspect that the generalization error\nscales as $\\tilde{\\mathcal{O}}(\\sqrt{MT/N})$ by directly applying the\nabove result (and where $\\tilde{\\mathcal{O}}$ includes logarithmic\nfactors). However, the authors of Ref. found that such models actually\nadhere to the better scaling\n\n$$\\mathrm{gen}(\\alpha) \\sim \\mathcal{O}\\left(\\sqrt{\\frac{T\\log MT}{N}}\\right).$$\n\nWith this, we see that for QCNNs to have a generalization error\n$\\mathrm{gen}(\\alpha)\\leq\\epsilon$, we need a training set of size\n$N \\sim T \\log MT / \\epsilon^2$. For the special case of QCNNs, we can\nexplicitly connect the number of samples needed for good generalization\nto the system size $n$ since these models use $\\mathcal{O}(\\log(n))$\nindependently parametrized gates, each of which is used at most $n$\ntimes. Putting the pieces together, we find that a training set of size\n\n$$N \\sim \\mathcal{O}(\\mathrm{poly}(\\log n))$$\n\nis sufficient for the generalization error to be bounded by\n$\\mathrm{gen}(\\alpha) \\leq \\epsilon$. In the next part of this tutorial,\nwe will illustrate this result by implementing a QCNN to classify\ndifferent digits in the classical `digits` dataset. Before that, we set\nup our QCNN.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quantum convolutional neural networks\n\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\--Before\nwe start building a QCNN, let us briefly review the idea of classical\nCNNs, which have shown tremendous success in tasks like image\nrecognition, recommender systems, and sound classification, to name a\nfew. For a more in-depth explanation of CNNs, we highly recommend\n[chapter 9](https://www.deeplearningbook.org/contents/convnets.html) in.\nClassical CNNs are a family of neural networks which make use of\nconvolutions and pooling operations to insert an inductive bias,\nfavouring invariances to spatial transformations like translations,\nrotations, and scaling. A *convolutional layer* consists of a small\nkernel (a window) that sweeps over a 2D array representation of an image\nand extracts local information while sharing parameters across the\nspatial dimensions. In addition to the convolutional layers, one\ntypically uses pooling layers to reduce the size of the input and to\nprovide a mechanism for summarizing information from a neighbourhood of\nvalues in the input. On top of reducing dimensionality, these types of\nlayers have the advantage of making the model more agnostic to certain\ntransformations like scaling and rotations. These two types of layers\nare applied repeatedly in an alternating manner as shown in the figure\nbelow.\n\n![A graphical representation of a CNN. Obtained using\nRef..](/demonstrations/learning_few_data/cnn_pic.png){.align-center\nwidth=\"75.0%\"}\n\nWe want to build something similar for a quantum circuit. First, we\nimport the necessary libraries we will need in this demo and set a\nrandom seed for reproducibility:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn import datasets\nimport seaborn as sns\nimport jax;\n\njax.config.update('jax_platform_name', 'cpu')\njax.config.update(\"jax_enable_x64\", True)\nimport jax.numpy as jnp\n\nimport optax  # optimization using jax\n\nimport pennylane as qml\nimport pennylane.numpy as pnp\n\nsns.set()\n\nseed = 0\nrng = np.random.default_rng(seed=seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To construct a convolutional and pooling layer in a quantum circuit, we\nwill follow the QCNN construction proposed in. The former layer will\nextract local correlations, while the latter allows reducing the\ndimensionality of the feature vector. In a quantum circuit, the\nconvolutional layer, consisting of a kernel swept along the entire\nimage, is a two-qubit unitary that correlates neighbouring qubits. As\nfor the pooling layer, we will use a conditioned single-qubit unitary\nthat depends on the measurement of a neighboring qubit. Finally, we use\na *dense layer* that entangles all qubits of the final state using an\nall-to-all unitary gate as shown in the figure below.\n\n![QCNN architecture. Taken from\nRef..](/demonstrations/learning_few_data/qcnn-architecture.png){.align-center\nwidth=\"75.0%\"}\n\nBreaking down the layers\n========================\n\nThe convolutional layer should have the weights of the two-qubit unitary\nas an input, which are updated at every training step. In PennyLane, we\nmodel this arbitrary two-qubit unitary with a particular sequence of\ngates: two single-qubit `~.pennylane.U3`{.interpreted-text role=\"class\"}\ngates (parametrized by three parameters, each), three Ising interactions\nbetween both qubits (each interaction is parametrized by one parameter),\nand two additional `~.pennylane.U3`{.interpreted-text role=\"class\"}\ngates on each of the two qubits.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def convolutional_layer(weights, wires, skip_first_layer=True):\n    \"\"\"Adds a convolutional layer to a circuit.\n    Args:\n        weights (np.array): 1D array with 15 weights of the parametrized gates.\n        wires (list[int]): Wires where the convolutional layer acts on.\n        skip_first_layer (bool): Skips the first two U3 gates of a layer.\n    \"\"\"\n    n_wires = len(wires)\n    assert n_wires >= 3, \"this circuit is too small!\"\n\n    for p in [0, 1]:\n        for indx, w in enumerate(wires):\n            if indx % 2 == p and indx < n_wires - 1:\n                if indx % 2 == 0 and not skip_first_layer:\n                    qml.U3(*weights[:3], wires=[w])\n                    qml.U3(*weights[3:6], wires=[wires[indx + 1]])\n                qml.IsingXX(weights[6], wires=[w, wires[indx + 1]])\n                qml.IsingYY(weights[7], wires=[w, wires[indx + 1]])\n                qml.IsingZZ(weights[8], wires=[w, wires[indx + 1]])\n                qml.U3(*weights[9:12], wires=[w])\n                qml.U3(*weights[12:], wires=[wires[indx + 1]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The pooling layer\\'s inputs are the weights of the single-qubit\nconditional unitaries, which in this case are\n`~.pennylane.U3`{.interpreted-text role=\"class\"} gates. Then, we apply\nthese conditional measurements to half of the unmeasured wires, reducing\nour system size by a factor of 2.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def pooling_layer(weights, wires):\n    \"\"\"Adds a pooling layer to a circuit.\n    Args:\n        weights (np.array): Array with the weights of the conditional U3 gate.\n        wires (list[int]): List of wires to apply the pooling layer on.\n    \"\"\"\n    n_wires = len(wires)\n    assert len(wires) >= 2, \"this circuit is too small!\"\n\n    for indx, w in enumerate(wires):\n        if indx % 2 == 1 and indx < n_wires:\n            m_outcome = qml.measure(w)\n            qml.cond(m_outcome, qml.U3)(*weights, wires=wires[indx - 1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can construct a QCNN by combining both layers and using an arbitrary\nunitary to model a dense layer. It will take a set of features \\-\\-- the\nimage \\-\\-- as input, encode these features using an embedding map,\napply rounds of convolutional and pooling layers, and eventually output\nthe desired measurement statistics of the circuit.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def conv_and_pooling(kernel_weights, n_wires, skip_first_layer=True):\n    \"\"\"Apply both the convolutional and pooling layer.\"\"\"\n    convolutional_layer(kernel_weights[:15], n_wires, skip_first_layer=skip_first_layer)\n    pooling_layer(kernel_weights[15:], n_wires)\n\n\ndef dense_layer(weights, wires):\n    \"\"\"Apply an arbitrary unitary gate to a specified set of wires.\"\"\"\n    qml.ArbitraryUnitary(weights, wires)\n\n\nnum_wires = 6\ndevice = qml.device(\"default.qubit\", wires=num_wires)\n\n\n@qml.qnode(device, interface=\"jax\")\ndef conv_net(weights, last_layer_weights, features):\n    \"\"\"Define the QCNN circuit\n    Args:\n        weights (np.array): Parameters of the convolution and pool layers.\n        last_layer_weights (np.array): Parameters of the last dense layer.\n        features (np.array): Input data to be embedded using AmplitudEmbedding.\"\"\"\n\n    layers = weights.shape[1]\n    wires = list(range(num_wires))\n\n    # inputs the state input_state\n    qml.AmplitudeEmbedding(features=features, wires=wires, pad_with=0.5)\n    qml.Barrier(wires=wires, only_visual=True)\n\n    # adds convolutional and pooling layers\n    for j in range(layers):\n        conv_and_pooling(weights[:, j], wires, skip_first_layer=(not j == 0))\n        wires = wires[::2]\n        qml.Barrier(wires=wires, only_visual=True)\n\n    assert last_layer_weights.size == 4 ** (len(wires)) - 1, (\n        \"The size of the last layer weights vector is incorrect!\"\n        f\" \\n Expected {4 ** (len(wires)) - 1}, Given {last_layer_weights.size}\"\n    )\n    dense_layer(last_layer_weights, wires)\n    return qml.probs(wires=(0))\n\n\nfig, ax = qml.draw_mpl(conv_net)(\n    np.random.rand(18, 2), np.random.rand(4 ** 2 - 1), np.random.rand(2 ** num_wires)\n)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the problem we will address, we need to encode 64 features in our\nquantum state. Thus, we require six qubits ($2^6 = 64$) to encode each\nfeature value in the amplitude of each computational basis state.\n\nTraining the QCNN on the digits dataset\n=======================================\n\nIn this demo, we are going to classify the digits `0` and `1` from the\nclassical `digits` dataset. Each hand-written digit image is represented\nas an $8 \\times 8$ array of pixels as shown below:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "digits = datasets.load_digits()\nimages, labels = digits.data, digits.target\n\nimages = images[np.where((labels == 0) | (labels == 1))]\nlabels = labels[np.where((labels == 0) | (labels == 1))]\n\nfig, axes = plt.subplots(nrows=1, ncols=12, figsize=(3, 1))\n\nfor i, ax in enumerate(axes.flatten()):\n    ax.imshow(images[i].reshape((8, 8)), cmap=\"gray\")\n    ax.axis(\"off\")\n\nplt.tight_layout()\nplt.subplots_adjust(wspace=0, hspace=0)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For convenience, we create a `load_digits_data` function that will make\nrandom training and testing sets from the `digits` dataset from\n`sklearn.dataset`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def load_digits_data(num_train, num_test, rng):\n    \"\"\"Return training and testing data of digits dataset.\"\"\"\n    digits = datasets.load_digits()\n    features, labels = digits.data, digits.target\n\n    # only use first two classes\n    features = features[np.where((labels == 0) | (labels == 1))]\n    labels = labels[np.where((labels == 0) | (labels == 1))]\n\n    # normalize data\n    features = features / np.linalg.norm(features, axis=1).reshape((-1, 1))\n\n    # subsample train and test split\n    train_indices = rng.choice(len(labels), num_train, replace=False)\n    test_indices = rng.choice(\n        np.setdiff1d(range(len(labels)), train_indices), num_test, replace=False\n    )\n\n    x_train, y_train = features[train_indices], labels[train_indices]\n    x_test, y_test = features[test_indices], labels[test_indices]\n\n    return (\n        jnp.asarray(x_train),\n        jnp.asarray(y_train),\n        jnp.asarray(x_test),\n        jnp.asarray(y_test),\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To optimize the weights of our variational model, we define the cost and\naccuracy functions to train and quantify the performance on the\nclassification task of the previously described QCNN:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@jax.jit\ndef compute_out(weights, weights_last, features, labels):\n    \"\"\"Computes the output of the corresponding label in the qcnn\"\"\"\n    cost = lambda weights, weights_last, feature, label: conv_net(weights, weights_last, feature)[\n        label\n    ]\n    return jax.vmap(cost, in_axes=(None, None, 0, 0), out_axes=0)(\n        weights, weights_last, features, labels\n    )\n\n\ndef compute_accuracy(weights, weights_last, features, labels):\n    \"\"\"Computes the accuracy over the provided features and labels\"\"\"\n    out = compute_out(weights, weights_last, features, labels)\n    return jnp.sum(out > 0.5) / len(out)\n\n\ndef compute_cost(weights, weights_last, features, labels):\n    \"\"\"Computes the cost over the provided features and labels\"\"\"\n    out = compute_out(weights, weights_last, features, labels)\n    return 1.0 - jnp.sum(out) / len(labels)\n\n\ndef init_weights():\n    \"\"\"Initializes random weights for the QCNN model.\"\"\"\n    weights = pnp.random.normal(loc=0, scale=1, size=(18, 2), requires_grad=True)\n    weights_last = pnp.random.normal(loc=0, scale=1, size=4 ** 2 - 1, requires_grad=True)\n    return jnp.array(weights), jnp.array(weights_last)\n\n\nvalue_and_grad = jax.jit(jax.value_and_grad(compute_cost, argnums=[0, 1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are going to perform the classification for training sets with\ndifferent values of $N$. Therefore, we define the classification\nprocedure once and then perform it for different datasets. Finally, we\nupdate the weights using the `pennylane.AdamOptimizer`{.interpreted-text\nrole=\"class\"} and use these updated weights to calculate the cost and\naccuracy on the testing and training set:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def train_qcnn(n_train, n_test, n_epochs):\n    \"\"\"\n    Args:\n        n_train  (int): number of training examples\n        n_test   (int): number of test examples\n        n_epochs (int): number of training epochs\n        desc  (string): displayed string during optimization\n\n    Returns:\n        dict: n_train,\n        steps,\n        train_cost_epochs,\n        train_acc_epochs,\n        test_cost_epochs,\n        test_acc_epochs\n\n    \"\"\"\n    # load data\n    x_train, y_train, x_test, y_test = load_digits_data(n_train, n_test, rng)\n\n    # init weights and optimizer\n    weights, weights_last = init_weights()\n\n    # learning rate decay\n    cosine_decay_scheduler = optax.cosine_decay_schedule(0.1, decay_steps=n_epochs, alpha=0.95)\n    optimizer = optax.adam(learning_rate=cosine_decay_scheduler)\n    opt_state = optimizer.init((weights, weights_last))\n\n    # data containers\n    train_cost_epochs, test_cost_epochs, train_acc_epochs, test_acc_epochs = [], [], [], []\n\n    for step in range(n_epochs):\n        # Training step with (adam) optimizer\n        train_cost, grad_circuit = value_and_grad(weights, weights_last, x_train, y_train)\n        updates, opt_state = optimizer.update(grad_circuit, opt_state)\n        weights, weights_last = optax.apply_updates((weights, weights_last), updates)\n\n        train_cost_epochs.append(train_cost)\n\n        # compute accuracy on training data\n        train_acc = compute_accuracy(weights, weights_last, x_train, y_train)\n        train_acc_epochs.append(train_acc)\n\n        # compute accuracy and cost on testing data\n        test_out = compute_out(weights, weights_last, x_test, y_test)\n        test_acc = jnp.sum(test_out > 0.5) / len(test_out)\n        test_acc_epochs.append(test_acc)\n        test_cost = 1.0 - jnp.sum(test_out) / len(test_out)\n        test_cost_epochs.append(test_cost)\n\n    return dict(\n        n_train=[n_train] * n_epochs,\n        step=np.arange(1, n_epochs + 1, dtype=int),\n        train_cost=train_cost_epochs,\n        train_acc=train_acc_epochs,\n        test_cost=test_cost_epochs,\n        test_acc=test_acc_epochs,\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.note}\n::: {.title}\nNote\n:::\n\nThere are some small intricacies for speeding up this code that are\nworth mentioning. We are using `jax` for our training because it allows\nfor\n[just-in-time](https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html)\n(`jit`) compilation. A function decorated with `@jax.jit` will be\ncompiled upon its first execution and cached for future executions. This\nmeans the first execution will take longer, but all subsequent\nexecutions are substantially faster. Further, we use `jax.vmap` to\nvectorize the execution of the QCNN over all input states, as opposed to\nlooping through the training and test set at every execution.\n:::\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training for different training set sizes yields different accuracies,\nas seen below. As we increase the training data size, the overall test\naccuracy, a proxy for the models\\' generalization capabilities,\nincreases:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_test = 100\nn_epochs = 100\nn_reps = 100\n\n\ndef run_iterations(n_train):\n    results_df = pd.DataFrame(\n        columns=[\"train_acc\", \"train_cost\", \"test_acc\", \"test_cost\", \"step\", \"n_train\"]\n    )\n\n    for _ in range(n_reps):\n        results = train_qcnn(n_train=n_train, n_test=n_test, n_epochs=n_epochs)\n        results_df = pd.concat(\n            [results_df, pd.DataFrame.from_dict(results)], axis=0, ignore_index=True\n        )\n\n    return results_df\n\n\n# run training for multiple sizes\ntrain_sizes = [2, 5, 10, 20, 40, 80]\nresults_df = run_iterations(n_train=2)\nfor n_train in train_sizes[1:]:\n    results_df = pd.concat([results_df, run_iterations(n_train=n_train)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we plot the loss and accuracy for both the training and testing\nset for all training epochs, and compare the test and train accuracy of\nthe model:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# aggregate dataframe\ndf_agg = results_df.groupby([\"n_train\", \"step\"]).agg([\"mean\", \"std\"])\ndf_agg = df_agg.reset_index()\n\nsns.set_style('whitegrid')\ncolors = sns.color_palette()\nfig, axes = plt.subplots(ncols=3, figsize=(16.5, 5))\n\ngeneralization_errors = []\n\n# plot losses and accuracies\nfor i, n_train in enumerate(train_sizes):\n    df = df_agg[df_agg.n_train == n_train]\n\n    dfs = [df.train_cost[\"mean\"], df.test_cost[\"mean\"], df.train_acc[\"mean\"], df.test_acc[\"mean\"]]\n    lines = [\"o-\", \"x--\", \"o-\", \"x--\"]\n    labels = [fr\"$N={n_train}$\", None, fr\"$N={n_train}$\", None]\n    axs = [0,0,2,2]\n    \n    for k in range(4):\n        ax = axes[axs[k]]   \n        ax.plot(df.step, dfs[k], lines[k], label=labels[k], markevery=10, color=colors[i], alpha=0.8)\n\n\n    # plot final loss difference\n    dif = df[df.step == 100].test_cost[\"mean\"] - df[df.step == 100].train_cost[\"mean\"]\n    generalization_errors.append(dif)\n\n# format loss plot\nax = axes[0]\nax.set_title('Train and Test Losses', fontsize=14)\nax.set_xlabel('Epoch')\nax.set_ylabel('Loss')\n\n# format generalization error plot\nax = axes[1]\nax.plot(train_sizes, generalization_errors, \"o-\", label=r\"$gen(\\alpha)$\")\nax.set_xscale('log')\nax.set_xticks(train_sizes)\nax.set_xticklabels(train_sizes)\nax.set_title(r'Generalization Error $gen(\\alpha) = R(\\alpha) - \\hat{R}_N(\\alpha)$', fontsize=14)\nax.set_xlabel('Training Set Size')\n\n# format loss plot\nax = axes[2]\nax.set_title('Train and Test Accuracies', fontsize=14)\nax.set_xlabel('Epoch')\nax.set_ylabel('Accuracy')\nax.set_ylim(0.5, 1.05)\n\nlegend_elements = [\n    mpl.lines.Line2D([0], [0], label=f'N={n}', color=colors[i]) for i, n in enumerate(train_sizes)\n    ] + [\n    mpl.lines.Line2D([0], [0], marker='o', ls='-', label='Train', color='Black'),\n    mpl.lines.Line2D([0], [0], marker='x', ls='--', label='Test', color='Black')\n    ]\n\naxes[0].legend(handles=legend_elements, ncol=3)\naxes[2].legend(handles=legend_elements, ncol=3)\n\naxes[1].set_yscale('log', base=2)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------------------------------------------------------------\n\nThe key takeaway of this work is that some quantum learning models can\nachieve high-fidelity predictions using a few training data points. We\nimplemented a model known as the quantum convolutional neural network\n(QCNN) using PennyLane for a binary classification task. Using six\nqubits, we have trained the QCNN to distinguish between handwritten\ndigits of $0$\\'s and $1$\\'s. With $80$ samples, we have achieved a model\nwith accuracy greater than $97\\%$ in $100$ training epochs. Furthermore,\nwe have compared the test and train accuracy of this model for a\ndifferent number of training samples and found the scaling of the\ngeneralization error agrees with the theoretical bounds obtained in[^1].\n\nReferences\n==========\n\nAbout the authors\n=================\n\n[^1]: Matthias C. Caro, Hsin-Yuan Huang, M. Cerezo, Kunal Sharma, Andrew\n    Sornborger, Lukasz Cincio, Patrick J. Coles. \\\"Generalization in\n    quantum machine learning from few training data\\\"\n    [arxiv:2111.05292](https://arxiv.org/abs/2111.05292), 2021.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}