{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# This cell is added by sphinx-gallery\n# It can be customized to whatever you like\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quantum detection of time series anomalies\n==========================================\n\n::: {.meta}\n:property=\\\"og:description\\\": Learn how to quantumly detect anomalous\nbehaviour in time series data with the help of Covalent.\n:property=\\\"og:image\\\":\n<https://pennylane.ai/qml/_images/thumbnail_tutorial_univariate_qvr.jpg>\n:::\n\n::: {.related}\ntutorial\\_qaoa\\_intro Intro to QAOA\n:::\n\n*Authors: Jack Stephen Baker, Santosh Kumar Radha --- Posted: 7 February\n2023.*\n\nSystems producing observable characteristics which evolve with time are\nalmost everywhere we look. The temperature changes as day turns to\nnight, stock markets fluctuate and the bacteria colony living in the\ncoffee cup to your right, which you *promised* you would clean\nyesterday, is slowly growing (seriously, clean it). In many situations,\nit is important to know when these systems start behaving abnormally.\nFor example, if the pressure inside a nuclear fission reactor starts\nviolently fluctuating, you may wish to be alerted of that. The task of\nidentifying such temporally abnormal behaviour is known as time series\nanomaly detection and is well known in machine learning circles.\n\nIn this tutorial, we take a stab at time series anomaly detection using\nthe *Quantum Variational Rewinding* algorithm, or QVR, proposed by\n[Baker, Horowitz, Radha et. al (2022)](https://arxiv.org/abs/2210.16438)\n--- a quantum machine learning algorithm for gate model quantum\ncomputers. QVR leverages the power of unitary time evolution/devolution\noperators to learn a model of *normal* behaviour for time series data.\nGiven a new (i.e., unseen in training) time series, the normal model\nproduces a value that, beyond a threshold, defines anomalous behaviour.\nIn this tutorial, we'll be showing you how all of this works, combining\nelements from [Covalent](https://www.covalent.xyz/),\n[Pennylane](https://pennylane.ai/) and [PyTorch](https://pytorch.org/).\n\nBefore getting into the technical details of the algorithm, let\\'s get a\nhigh-level overview with the help of the cartoon below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](../demonstrations/univariate_qvr/cartoon_pennylane.png){.align-center\nwidth=\"70.0%\"}\n\nGoing left-to-right, a time series is sampled at three points in time,\ncorresponding to different stages in the life cycle of a butterfly: a\ncatepillar, a chrysalis and a butterfly. This information is then\nencoded into quantum states and passed to a time machine which time\ndevolves the states as generated by a learnt Hamiltonian operator (in\npractice, there is a distribution of such operators). After the devolved\nstate is measured, the time series is recognized as normal if the\naverage measurement is smaller than a given threshold and anomalous if\nthe threshold is exceeded. In the first case, the time series is\nconsidered rewindable, correctly recovering the initial condition for\nthe life cycle of a butterfly: eggs on a leaf. In the second case, the\noutput is unrecognizable.\n\nThis will all make more sense once we delve into the math a little.\nLet\\'s do it!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Background\n==========\n\nTo begin, let's quickly recount the data that QVR handles: time series.\nA general time series $\\boldsymbol{y}$ can be described as a sequence of\n$p$-many observations of a process/system arranged in chronological\norder, where $p$ is a positive integer:\n\n$$\\boldsymbol{y} := (\\boldsymbol{y}_t: t \\in T), \\quad T := (t_l: l \\in \\mathbb{Z}^{+}_{\\leq p}).$$\n\nIn the simple and didactic case treated in this tutorial,\n$\\boldsymbol{y}$ is univariate (i.e, is a one-dimensional time series),\nso bold-face for $\\boldsymbol{y}$ is dropped from this point onwards.\nAlso, we take $y_t \\in \\mathbb{R}$ and $t_l \\in \\mathbb{R}_{>0}$.\n\nThe goal of QVR and many other (classical) machine learning algorithms\nfor time series anomaly detection is to determine a suitable *anomaly\nscore* function $a_{X}$, where $X$ is a training dataset of *normal*\ntime series instances $x \\in X$ ($x$ is defined analogously to $y$ in\nthe above), from which the anomaly score function was learnt. When\npassed a general time series $y$, this function produces a real number:\n$a_X(y) \\in \\mathbb{R}$. The goal is to have $a_X(x) \\approx 0$, for all\n$x \\in X$. Then, for an unseen time series $y$ and a threshold\n$\\zeta \\in \\mathbb{R}$, the series is said to be anomalous should\n$a_X(y) > \\zeta,$ and normal otherwise. We show a strategy for setting\n$\\zeta$ later in this tutorial.\n\nThe first step for doing all of this *quantumly* is to generate a\nsequence $\\mathcal{S} := (|x_{t} \\rangle: t \\in T)$ of $n$-qubit quantum\nstates corresponding to a classical time series instance in the training\nset. Now, we suppose that each $|x_t \\rangle$ is a quantum state evolved\nto a time $t$, as generated by an *unknown embedding Hamiltonian* $H_E$.\nThat is, each element of $\\mathcal{S}$ is defined by\n$|x_t \\rangle = e^{-iH_E(x_t)}|0\\rangle^{\\otimes n} = U(x_t)|0\\rangle^{\\otimes n}$\nfor an embedding unitary operator $U(x_t)$ implementing a quantum\nfeature map (see the [Pennylane embedding\ntemplates](https://docs.pennylane.ai/en/stable/introduction/templates.html#embedding-templates)\nfor efficient quantum circuits for doing so). Next, we operate on each\n$|x_t\\rangle$ with a parameterized\n$e^{-iH(\\boldsymbol{\\alpha}, \\boldsymbol{\\gamma})t}$ operator to prepare\nthe states\n\n$$|x_t, \\boldsymbol{\\alpha}, \\boldsymbol{\\gamma}\\rangle := e^{-iH(\\boldsymbol{\\alpha}, \\boldsymbol{\\gamma})t}|x_t\\rangle,$$\n\nwhere we write $e^{-iH(\\boldsymbol{\\alpha}, \\boldsymbol{\\gamma})t}$ as\nan eigendecomposition\n\n$$V_t(\\boldsymbol{\\alpha}, \\boldsymbol{\\gamma}) := W^{\\dagger}(\\boldsymbol{\\alpha})D(\\boldsymbol{\\gamma}, t)W(\\boldsymbol{\\alpha}) = e^{-iH(\\boldsymbol{\\alpha}, \\boldsymbol{\\gamma})t}.$$\n\nHere, the unitary matrix of eigenvectors $W(\\boldsymbol{\\alpha})$ is\nparametrized by $\\boldsymbol{\\alpha}$ and the unitary diagonalization\n$D(\\boldsymbol{\\gamma}, t)$ is parametrized by $\\boldsymbol{\\gamma}.$\nBoth can be implemented efficiently using parameterized quantum\ncircuits. The above equality with\n$e^{-iH(\\boldsymbol{\\alpha}, \\boldsymbol{\\gamma})t}$ is a consequence of\nStone's theorem for strongly continuous one-parameter unitary groups.\n\nWe now ask the question: *What condition is required for*\n$|x_t, \\boldsymbol{\\alpha}, \\boldsymbol{\\gamma} \\rangle = |0 \\rangle^{\\otimes n}$\n*for all time?* To answer this, we impose\n$P(|0\\rangle^{\\otimes n}) = |\\langle 0|^{\\otimes n}|x_t, \\boldsymbol{\\alpha}, \\boldsymbol{\\gamma} \\rangle|^2 = 1.$\nPlaying with the algebra a little, we find that the following condition\nmust be satisfied for all $t$:\n\n$$\\langle 0|^{\\otimes n}e^{-iH(\\boldsymbol{\\alpha}, \\boldsymbol{\\gamma})t}e^{-iH_E(x_t)}|0\\rangle^{\\otimes n} = 1 \\iff  H(\\boldsymbol{\\alpha}, \\boldsymbol{\\gamma})t = -H_E(x_t).$$\n\nIn other words, for the above to be true, the parameterized unitary\noperator $V_t(\\boldsymbol{\\alpha}, \\boldsymbol{\\gamma})$ should be able\nto reverse or *rewind* $|x_t\\rangle$ to its initial state\n$|0\\rangle^{\\otimes n}$ before the embedding unitary operator $U(x_t)$\nwas applied.\n\nWe are nearly there! Because it is reasonable to expect that a single\nHamiltonian will not be able to successfully rewind every $x \\in X$ (in\nfact, this is impossible to do if each $x$ is unique, which is usually\ntrue), we consider the average effect of many Hamiltonians generated by\ndrawing $\\boldsymbol{\\gamma}$ from a normal distribution\n$\\mathcal{N}(\\mu, \\sigma)$ with mean $\\mu$ and standard deviation\n$\\sigma$:\n\n$$F(\\boldsymbol{\\phi}, x_t) := \\mathop{\\mathbb{E}_{\\boldsymbol{\\gamma} \\sim \\mathcal{N}(\\mu, \\sigma)}}\\left[\\langle 0|^{\\otimes n} |x_t, \\boldsymbol{\\alpha}, \\boldsymbol{\\gamma}\\rangle  \\right], \\quad \\boldsymbol{\\phi} = [\\boldsymbol{\\alpha}, \\mu, \\sigma].$$\n\nThe goal is for the function $F$ defined above to be as close to $1$ as\npossible, for all $x \\in X$ and $t \\in T.$ With this in mind, we can\ndefine the loss function to minimize as the mean square error\nregularized by a penalty function $P_{\\tau}(\\sigma)$ with a single\nhyperparameter $\\tau$:\n\n$$\\mathcal{L(\\boldsymbol{\\phi})} = \\frac{1}{2|X||T|}\\sum_{x \\in X} \\sum_{t \\in T}[1 - F(\\boldsymbol{\\phi}, x_t)]^2 + P_{\\tau}(\\sigma).$$\n\nWe will show the exact form of $P_{\\tau}(\\sigma)$ later. The general\npurpose of the penalty function is to penalize large values of $\\sigma$\n(justification for this is given in the Supplement of). After\napproximately finding the argument $\\boldsymbol{\\phi}^{\\star}$ that\nminimizes the loss function (found using a classical optimization\nroutine), we finally arrive at a definition for our anomaly score\nfunction $a_X(y)$\n\n$$a_X(y) = \\frac{1}{|T|}\\sum_{t \\in T}[1 - F(\\boldsymbol{\\phi}^{\\star}, y_t)]^2.$$\n\nIt may now be apparent that we have implemented a clustering algorithm!\nThat is, our model $F$ was trained such that normal time series\n$x \\in X$ produce $F(\\boldsymbol{\\phi}^{\\star}, x_t)$ clustered about a\ncenter at $1$. Given a new time series $y$, should\n$F(\\boldsymbol{\\phi}^{\\star}, y_t)$ venture far from the normal center\nat $1$, we are observing anomalous behaviour!\n\nTake the time now to have another look at the cartoon at the start of\nthis tutorial. Hopefully things should start making sense now.\n\nNow with our algorithm defined, let's stitch this all together: enter\n[Covalent](https://www.covalent.xyz/).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Covalent: heterogeneous workflow orchestration\n==============================================\n\nPresently, many QML algorithms are *heterogeneous* in nature. This means\nthat they require computational resources from both classical and\nquantum computing. Covalent is a tool that can be used to manage their\ninteraction by sending different tasks to different computational\nresources and stitching them together as a workflow. While you will be\nintroduced to other concepts in Covalent throughout this tutorial, we\ndefine two key components to begin with.\n\n1.  **Electrons**. Decorate regular Python functions with `@ct.electron`\n    to desginate a *task*. These are the atoms of a computation.\n\n2.  **Lattices**. Decorate a regular Python function with `@ct.lattice`\n    to designate a *workflow*. These contain electrons stitched together\n    to do something useful.\n\n    Different electrons can be run remotely on different hardware and\n    multiple computational paridigms (classical, quantum, etc.: see the\n    [Covalent\n    executors](https://covalent.readthedocs.io/en/stable/plugins.html)).\n    In this tutorial, however, to keep things simple, tasks are run on a\n    local Dask cluster, which provides (among other things)\n    auto-parallelization.\n\n![\\| A schematic demonstrating the different platforms Covalent can\ninteract\nwith.](../demonstrations/univariate_qvr/covalent_platform.png){.align-center\nwidth=\"70.0%\"}\n\nNow is a good time to import Covalent and launch the Covalent server!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import covalent as ct\nimport os\nimport time\n\n# Set up Covalent server\nos.environ[\"COVALENT_SERVER_IFACE_ANY\"] = \"1\"\nos.system(\"covalent start\")\n# If you run into any out-of-memory issues with Dask when running this notebook,\n# Try reducing the number of workers and making a specific memory request. I.e.:\n# os.system(\"covalent start -m \"2GiB\" -n 2\")\n# try covalent --help for more info\ntime.sleep(2)  # give the Dask cluster some time to launch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generating univariate synthetic time series\n===========================================\n\nIn this tutorial, we shall deal with a simple and didactic example.\nNormal time series instances are chosen to be noisy low-amplitude\nsignals normally distributed about the origin. In our case,\n$x_t \\sim \\mathcal{N}(0, 0.1)$. Series we deem to be anomalous are the\nsame but with randomly inserted spikes with random durations and\namplitudes.\n\nLet's make a `@ct.electron` to generate each of these synthetic time\nseries sets. For this, we\\'ll need to import Torch. We\\'ll also set the\ndefault tensor type and pick a random seed for the whole tutorial for\nreproducibility.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\n\n# Seed Torch for reproducibility and set default tensor type\nGLOBAL_SEED = 1989\ntorch.manual_seed(GLOBAL_SEED)\ntorch.set_default_tensor_type(torch.DoubleTensor)\n\n\n@ct.electron\ndef generate_normal_time_series_set(\n    p: int, num_series: int, noise_amp: float, t_init: float, t_end: float, seed: int = GLOBAL_SEED\n) -> tuple:\n    \"\"\"Generate a normal time series data set where each of the p elements\n    is drawn from a normal distribution x_t ~ N(0, noise_amp).\n    \"\"\"\n    torch.manual_seed(seed)\n    X = torch.normal(0, noise_amp, (num_series, p))\n    T = torch.linspace(t_init, t_end, p)\n    return X, T\n\n\n@ct.electron\ndef generate_anomalous_time_series_set(\n    p: int,\n    num_series: int,\n    noise_amp: float,\n    spike_amp: float,\n    max_duration: int,\n    t_init: float,\n    t_end: float,\n    seed: int = GLOBAL_SEED,\n) -> tuple:\n    \"\"\"Generate an anomalous time series data set where the p elements of each sequence are\n    from a normal distribution x_t ~ N(0, noise_amp). Then,\n    anomalous spikes of random amplitudes and durations are inserted.\n    \"\"\"\n    torch.manual_seed(seed)\n    Y = torch.normal(0, noise_amp, (num_series, p))\n    for y in Y:\n        # 5\u201310 spikes allowed\n        spike_num = torch.randint(low=5, high=10, size=())\n        durations = torch.randint(low=1, high=max_duration, size=(spike_num,))\n        spike_start_idxs = torch.randperm(p - max_duration)[:spike_num]\n        for start_idx, duration in zip(spike_start_idxs, durations):\n            y[start_idx : start_idx + duration] += torch.normal(0.0, spike_amp, (duration,))\n    T = torch.linspace(t_init, t_end, p)\n    return Y, T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\\'s do a quick sanity check and plot a couple of these series.\nDespite the above function\\'s `@ct.electron` decorators, these can still\nbe used as normal Python functions without using the Covalent server.\nThis is useful for quick checks like this:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\nX_norm, T_norm = generate_normal_time_series_set(25, 25, 0.1, 0.1, 2 * torch.pi)\nY_anom, T_anom = generate_anomalous_time_series_set(25, 25, 0.1, 0.4, 5, 0, 2 * torch.pi)\n\nplt.figure()\nplt.plot(T_norm, X_norm[0], label=\"Normal\")\nplt.plot(T_anom, Y_anom[1], label=\"Anomalous\")\nplt.ylabel(\"$y(t)$\")\nplt.xlabel(\"t\")\nplt.grid()\nleg = plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Taking a look at the above, the generated series are what we wanted. We\nhave a simple human-parsable notion of what it is for a time series to\nbe anomalous (big spikes). Of course, we don\\'t need a complicated\nalgorithm to be able to detect such anomalies but this is just a\ndidactic example remember!\n\nLike many machine learning algorithms, training is done in mini-batches.\nExamining the form of the loss function\n$\\mathcal{L}(\\boldsymbol{\\phi})$, we can see that time series are\natomized. In other words, each term in the mean square error is for a\ngiven $x_t$ and not measured against the entire series $x$. This allows\nus to break down the training set $X$ into time-series-independent\nchunks. Here's an electron to do that:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@ct.electron\ndef make_atomized_training_set(X: torch.Tensor, T: torch.Tensor) -> list:\n    \"\"\"Convert input time series data provided in a two-dimensional tensor format\n    to atomized tuple chunks: (xt, t).\n    \"\"\"\n    X_flat = torch.flatten(X)\n    T_flat = T.repeat(X.size()[0])\n    atomized = [(xt, t) for xt, t in zip(X_flat, T_flat)]\n    return atomized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now wish to pass this to a cycled `torch.utils.data.DataLoader`.\nHowever, this object is not\n[pickleable](https://docs.python.org/3/library/pickle.html#:~:text=%E2%80%9CPickling%E2%80%9D%20is%20the%20process%20whereby,back%20into%20an%20object%20hierarchy.),\nwhich is a requirement of electrons in Covalent. We therefore use the\nbelow helper class to create a pickleable version.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from collections.abc import Iterator\n\n\nclass DataGetter:\n    \"\"\"A pickleable mock-up of a Python iterator on a torch.utils.Dataloader.\n    Provide a dataset X and the resulting object O will allow you to use next(O).\n    \"\"\"\n\n    def __init__(self, X: torch.Tensor, batch_size: int, seed: int = GLOBAL_SEED) -> None:\n        \"\"\"Calls the _init_data method on intialization of a DataGetter object.\"\"\"\n        torch.manual_seed(seed)\n        self.X = X\n        self.batch_size = batch_size\n        self.data = []\n        self._init_data(\n            iter(torch.utils.data.DataLoader(self.X, batch_size=self.batch_size, shuffle=True))\n        )\n\n    def _init_data(self, iterator: Iterator) -> None:\n        \"\"\"Load all of the iterator into a list.\"\"\"\n        x = next(iterator, None)\n        while x is not None:\n            self.data.append(x)\n            x = next(iterator, None)\n\n    def __next__(self) -> tuple:\n        \"\"\"Analogous behaviour to the native Python next() but calling the\n        .pop() of the data attribute.\n        \"\"\"\n        try:\n            return self.data.pop()\n        except IndexError:  # Caught when the data set runs out of elements\n            self._init_data(\n                iter(torch.utils.data.DataLoader(self.X, batch_size=self.batch_size, shuffle=True))\n            )\n            return self.data.pop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We call an instance of the above in an electron\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@ct.electron\ndef get_training_cycler(Xtr: torch.Tensor, batch_size: int, seed: int = GLOBAL_SEED) -> DataGetter:\n    \"\"\"Get an instance of the DataGetter class defined above, which behaves analogously to\n    next(iterator) but is pickleable.\n    \"\"\"\n    return DataGetter(Xtr, batch_size, seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now have the means to create synthetic data and cycle through a\ntraining set. Next, we need to build our loss function\n$\\mathcal{L}(\\boldsymbol{\\phi})$ from electrons with the help of\n`PennyLane`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Building the loss function\n==========================\n\nCore to building the loss function is the quantum circuit implementing\n$V_t(\\boldsymbol{\\alpha}, \\boldsymbol{\\gamma}) := W^{\\dagger}(\\boldsymbol{\\alpha})D(\\boldsymbol{\\gamma}, t)W(\\boldsymbol{\\alpha})$.\nWhile there are existing templates in `PennyLane` for implementing\n$W(\\boldsymbol{\\alpha})$, we use a custom circuit to implement\n$D(\\boldsymbol{\\gamma}, t)$. Following the approach taken in (also\nexplained in and the appendix of), we create the electron:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pennylane as qml\nfrom itertools import combinations\n\n\n@ct.electron\ndef D(gamma: torch.Tensor, n_qubits: int, k: int = None, get_probs: bool = False) -> None:\n    \"\"\"Generates an n_qubit quantum circuit according to a k-local Walsh operator\n    expansion. Here, k-local means that 1 <= k <= n of the n qubits can interact.\n    See <https://doi.org/10.1088/1367-2630/16/3/033040> for more\n    details. Optionally return probabilities of bit strings.\n    \"\"\"\n    if k is None:\n        k = n_qubits\n    cnt = 0\n    for i in range(1, k + 1):\n        for comb in combinations(range(n_qubits), i):\n            if len(comb) == 1:\n                qml.RZ(gamma[cnt], wires=[comb[0]])\n                cnt += 1\n            elif len(comb) > 1:\n                cnots = [comb[i : i + 2] for i in range(len(comb) - 1)]\n                for j in cnots:\n                    qml.CNOT(wires=j)\n                qml.RZ(gamma[cnt], wires=[comb[-1]])\n                cnt += 1\n                for j in cnots[::-1]:\n                    qml.CNOT(wires=j)\n    if get_probs:\n        return qml.probs(wires=range(n_qubits))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "While the above may seem a little complicated, since we only use a\nsingle qubit in this tutorial, the resulting circuit is merely a single\n$R_z(\\theta)$ gate.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_qubits = 1\ndev = qml.device(\"default.qubit\", wires=n_qubits, shots=None)\nD_one_qubit = qml.qnode(dev)(D)\n_ = qml.draw_mpl(D_one_qubit, decimals=2)(torch.tensor([1, 0]), 1, 1, True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You may find the general function for $D$\\` useful in case you want to\nexperiment with more qubits and your own (possibly multi-dimensional)\ndata after this tutorial.\n\nNext, we define a circuit to calculate the probability of certain bit\nstrings being measured in the computational basis. In our simple\nexample, we work only with one qubit and use the `default.qubit` local\nquantum circuit simulator.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@ct.electron\n@qml.qnode(dev, interface=\"torch\", diff_method=\"backprop\")\ndef get_probs(\n    xt: torch.Tensor,\n    t: float,\n    alpha: torch.Tensor,\n    gamma: torch.Tensor,\n    k: int,\n    U: callable,\n    W: callable,\n    D: callable,\n    n_qubits: int,\n) -> torch.Tensor:\n    \"\"\"Measure the probabilities for measuring each bitstring after applying a\n    circuit of the form W\u2020DWU to the |0\u27e9^(\u2297n) state. This\n    function is defined for individual sequence elements xt.\n    \"\"\"\n    U(xt, wires=range(n_qubits))\n    W(alpha, wires=range(n_qubits))\n    D(gamma * t, n_qubits, k)\n    qml.adjoint(W)(alpha, wires=range(n_qubits))\n    return qml.probs(range(n_qubits))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To take the projector $|0\\rangle^{\\otimes n} \\langle 0 |^{\\otimes n}$,\nwe consider only the probability of measuring the bit string of all\nzeroes, which is the 0th element of the probabilities (bit strings are\nreturned in lexicographic order).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@ct.electron\ndef get_callable_projector_func(\n    k: int, U: callable, W: callable, D: callable, n_qubits: int, probs_func: callable\n) -> callable:\n    \"\"\"Using get_probs() above, take only the probability of measuring the\n    bitstring of all zeroes (i.e, take the projector\n    |0\u27e9^(\u2297n)\u27e80|^(\u2297n)) on the time devolved state.\n    \"\"\"\n    callable_proj = lambda xt, t, alpha, gamma: probs_func(\n        xt, t, alpha, gamma, k, U, W, D, n_qubits\n    )[0]\n    return callable_proj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now have the necessary ingredients to build\n$F(\\boldsymbol{\\phi}, x_t)$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@ct.electron\ndef F(\n    callable_proj: callable,\n    xt: torch.Tensor,\n    t: float,\n    alpha: torch.Tensor,\n    mu: torch.Tensor,\n    sigma: torch.Tensor,\n    gamma_length: int,\n    n_samples: int,\n) -> torch.Tensor:\n    \"\"\"Take the classical expecation value of of the projector on zero sampling\n    the parameters of D from normal distributions. The expecation value is estimated\n    with an average over n_samples.\n    \"\"\"\n    # length of gamma should not exceed 2^n - 1\n    gammas = sigma.abs() * torch.randn((n_samples, gamma_length)) + mu\n    expectation = torch.empty(n_samples)\n    for i, gamma in enumerate(gammas):\n        expectation[i] = callable_proj(xt, t, alpha, gamma)\n    return expectation.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now return to the matter of the penalty function $P_{\\tau}$. We\nchoose\n\n$$P_{\\tau}(\\sigma) := \\frac{1}{\\pi} \\arctan(2 \\pi \\tau |\\sigma|).$$\n\nAs an electron, we have\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@ct.electron\ndef callable_arctan_penalty(tau: float) -> callable:\n    \"\"\"Create a callable arctan function with a single hyperparameter\n    tau to penalize large entries of sigma.\n    \"\"\"\n    prefac = 1 / (torch.pi)\n    callable_pen = lambda sigma: prefac * torch.arctan(2 * torch.pi * tau * sigma.abs()).mean()\n    return callable_pen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The above is a sigmoidal function chosen because it comes with the\nuseful property of being bounded. The prefactor of $1/\\pi$ is chosen\nsuch that the final loss $\\mathcal{L}(\\boldsymbol{\\phi})$ is defined in\nthe range (0, 1), as defined in the below electron.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@ct.electron\ndef get_loss(\n    callable_proj: callable,\n    batch: torch.Tensor,\n    alpha: torch.Tensor,\n    mu: torch.Tensor,\n    sigma: torch.Tensor,\n    gamma_length: int,\n    n_samples: int,\n    callable_penalty: callable,\n) -> torch.Tensor:\n    \"\"\"Evaluate the loss function \u2112, defined in the background section\n    for a certain set of parameters.\n    \"\"\"\n    X_batch, T_batch = batch\n    loss = torch.empty(X_batch.size()[0])\n    for i in range(X_batch.size()[0]):\n        # unsqueeze required for tensor to have the correct dimension for PennyLane templates\n        loss[i] = (\n            1\n            - F(\n                callable_proj,\n                X_batch[i].unsqueeze(0),\n                T_batch[i].unsqueeze(0),\n                alpha,\n                mu,\n                sigma,\n                gamma_length,\n                n_samples,\n            )\n        ).square()\n    return 0.5 * loss.mean() + callable_penalty(sigma)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training the normal model\n=========================\n\nNow equipped with a loss function, we need to minimize it with a\nclassical optimization routine. To start this optimization, however, we\nneed some initial parameters. We can generate them with the below\nelectron.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@ct.electron\ndef get_initial_parameters(\n    W: callable, W_layers: int, n_qubits: int, seed: int = GLOBAL_SEED\n) -> dict:\n    \"\"\"Randomly generate initial parameters. We need initial parameters for the\n    variational circuit ansatz implementing W(alpha) and the standard deviation\n    and mean (sigma and mu) for the normal distribution we sample gamma from.\n    \"\"\"\n    torch.manual_seed(seed)\n    init_alpha = torch.rand(W.shape(W_layers, n_qubits))\n    init_mu = torch.rand(1)\n    # Best to start sigma small and expand if needed\n    init_sigma = torch.rand(1)\n    init_params = {\n        \"alpha\": (2 * torch.pi * init_alpha).clone().detach().requires_grad_(True),\n        \"mu\": (2 * torch.pi * init_mu).clone().detach().requires_grad_(True),\n        \"sigma\": (0.1 * init_sigma + 0.05).clone().detach().requires_grad_(True),\n    }\n    return init_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the `PyTorch` interface to `PennyLane`, we define our final\nelectron before running the training workflow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@ct.electron\ndef train_model_gradients(\n    lr: float,\n    init_params: dict,\n    pytorch_optimizer: callable,\n    cycler: DataGetter,\n    n_samples: int,\n    callable_penalty: callable,\n    batch_iterations: int,\n    callable_proj: callable,\n    gamma_length: int,\n    seed=GLOBAL_SEED,\n    print_intermediate=False,\n) -> dict:\n    \"\"\"Train the QVR model (minimize the loss function) with respect to the\n    variational parameters using gradient-based training. You need to pass a\n    PyTorch optimizer and a learning rate (lr).\n    \"\"\"\n    torch.manual_seed(seed)\n    opt = pytorch_optimizer(init_params.values(), lr=lr)\n    alpha = init_params[\"alpha\"]\n    mu = init_params[\"mu\"]\n    sigma = init_params[\"sigma\"]\n\n    def closure():\n        opt.zero_grad()\n        loss = get_loss(\n            callable_proj, next(cycler), alpha, mu, sigma, gamma_length, n_samples, callable_penalty\n        )\n        loss.backward()\n        return loss\n\n    loss_history = []\n    for i in range(batch_iterations):\n        loss = opt.step(closure)\n        loss_history.append(loss.item())\n        if batch_iterations % 10 == 0 and print_intermediate:\n            print(f\"Iteration number {i}\\n Current loss {loss.item()}\\n\")\n\n    results_dict = {\n        \"opt_params\": {\n            \"alpha\": opt.param_groups[0][\"params\"][0],\n            \"mu\": opt.param_groups[0][\"params\"][1],\n            \"sigma\": opt.param_groups[0][\"params\"][2],\n        },\n        \"loss_history\": loss_history,\n    }\n    return results_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, enter our first `@ct.lattice`. This combines the above electrons,\neventually returning the optimal parameters $\\boldsymbol{\\phi}^{\\star}$\nand the loss with batch iterations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@ct.lattice\ndef training_workflow(\n    U: callable,\n    W: callable,\n    D: callable,\n    n_qubits: int,\n    k: int,\n    probs_func: callable,\n    W_layers: int,\n    gamma_length: int,\n    n_samples: int,\n    p: int,\n    num_series: int,\n    noise_amp: float,\n    t_init: float,\n    t_end: float,\n    batch_size: int,\n    tau: float,\n    pytorch_optimizer: callable,\n    lr: float,\n    batch_iterations: int,\n):\n    \"\"\"\n    Combine all of the previously defined electrons to do an entire training workflow,\n    including (1) generating synthetic data, (2) packaging it into training cyclers\n    (3) preparing the quantum functions and (4) optimizing the loss function with\n    gradient based optimization. You can find definitions for all of the arguments\n    by looking at the electrons and text cells above.\n    \"\"\"\n\n    X, T = generate_normal_time_series_set(p, num_series, noise_amp, t_init, t_end)\n    Xtr = make_atomized_training_set(X, T)\n    cycler = get_training_cycler(Xtr, batch_size)\n    init_params = get_initial_parameters(W, W_layers, n_qubits)\n    callable_penalty = callable_arctan_penalty(tau)\n    callable_proj = get_callable_projector_func(k, U, W, D, n_qubits, probs_func)\n    results_dict = train_model_gradients(\n        lr,\n        init_params,\n        pytorch_optimizer,\n        cycler,\n        n_samples,\n        callable_penalty,\n        batch_iterations,\n        callable_proj,\n        gamma_length,\n        print_intermediate=False,\n    )\n    return results_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before running this workflow, we define all of the input parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "general_options = {\n    \"U\": qml.AngleEmbedding,\n    \"W\": qml.StronglyEntanglingLayers,\n    \"D\": D,\n    \"n_qubits\": 1,\n    \"probs_func\": get_probs,\n    \"gamma_length\": 1,\n    \"n_samples\": 10,\n    \"p\": 25,\n    \"num_series\": 25,\n    \"noise_amp\": 0.1,\n    \"t_init\": 0.1,\n    \"t_end\": 2 * torch.pi,\n    \"k\": 1,\n}\n\ntraining_options = {\n    \"batch_size\": 10,\n    \"tau\": 5,\n    \"pytorch_optimizer\": torch.optim.Adam,\n    \"lr\": 0.01,\n    \"batch_iterations\": 100,\n    \"W_layers\": 2,\n}\n\ntraining_options.update(general_options)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now dispatch the lattice to the Covalent server.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tr_dispatch_id = ct.dispatch(training_workflow)(**training_options)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you are running the notebook version of this tutorial, if you\nnavigate to <http://localhost:48008/> you can view the workflow on the\nCovalent GUI. It will look like the screenshot below, showing nicely how\nall of the electrons defined above interact with each other in the\nworkflow. You can also track the progress of the calculation here.\n\n![A screenshot of the Covalent GUI for the training\nworkflow.](../demonstrations/univariate_qvr/covalent_tutorial_screenshot.png){.align-center\nwidth=\"85.0%\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now pull the results back from the Covalent server:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ct_tr_results = ct.get_result(dispatch_id=tr_dispatch_id, wait=True)\nresults_dict = ct_tr_results.result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and take a look at the training loss history:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure()\nplt.plot(results_dict[\"loss_history\"], \".-\")\nplt.ylabel(\"Loss [$\\mathcal{L}$]\")\nplt.xlabel(\"Batch iterations\")\nplt.title(\"Loss function versus batch iterations in training\")\nplt.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tuning the threshold $\\zeta$\n============================\n\nWhen we have access to labelled anomalous series (as we do in our toy\nproblem here, often not the case in reality), we can tune the threshold\n$\\zeta$ to maximize some success metric. We choose to maximize the\naccuracy score as defined using the three electrons below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@ct.electron\ndef get_preds_given_threshold(zeta: float, scores: torch.Tensor) -> torch.Tensor:\n    \"\"\"For a given threshold, get the predicted labels (1 or -1), given the anomaly scores.\"\"\"\n    return torch.tensor([-1 if score > zeta else 1 for score in scores])\n\n\n@ct.electron\ndef get_truth_labels(\n    normal_series_set: torch.Tensor, anomalous_series_set: torch.Tensor\n) -> torch.Tensor:\n    \"\"\"Get a 1D tensor containing the truth values (1 or -1) for a given set of\n    time series.\n    \"\"\"\n    norm = torch.ones(normal_series_set.size()[0])\n    anom = -torch.ones(anomalous_series_set.size()[0])\n    return torch.cat([norm, anom])\n\n\n@ct.electron\ndef get_accuracy_score(pred: torch.Tensor, truth: torch.Tensor) -> torch.Tensor:\n    \"\"\"Given the predictions and truth values, return a number between 0 and 1\n    indicating the accuracy of predictions.\n    \"\"\"\n    return torch.sum(pred == truth) / truth.size()[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, knowing the anomaly scores $a_X(y)$ for a validation data set, we\ncan scan through various values of $\\zeta$ on a fine 1D grid and\ncalcuate the accuracy score. Our goal is to pick the $\\zeta$ with the\nlargest accuracy score.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@ct.electron\ndef threshold_scan_acc_score(\n    scores: torch.Tensor, truth_labels: torch.Tensor, zeta_min: float, zeta_max: float, steps: int\n) -> torch.Tensor:\n    \"\"\"Given the anomaly scores and truth values,\n    scan over a range of thresholds = [zeta_min, zeta_max] with a\n    fixed number of steps, calculating the accuracy score at each point.\n    \"\"\"\n    accs = torch.empty(steps)\n    for i, zeta in enumerate(torch.linspace(zeta_min, zeta_max, steps)):\n        preds = get_preds_given_threshold(zeta, scores)\n        accs[i] = get_accuracy_score(preds, truth_labels)\n    return accs\n\n\n@ct.electron\ndef get_anomaly_score(\n    callable_proj: callable,\n    y: torch.Tensor,\n    T: torch.Tensor,\n    alpha_star: torch.Tensor,\n    mu_star: torch.Tensor,\n    sigma_star: torch.Tensor,\n    gamma_length: int,\n    n_samples: int,\n    get_time_resolved: bool = False,\n):\n    \"\"\"Get the anomaly score for an input time series y. We need to pass the\n    optimal parameters (arguments with suffix _star). Optionally return the\n    time-resolved score (the anomaly score contribution at a given t).\n    \"\"\"\n    scores = torch.empty(T.size()[0])\n    for i in range(T.size()[0]):\n        scores[i] = (\n            1\n            - F(\n                callable_proj,\n                y[i].unsqueeze(0),\n                T[i].unsqueeze(0),\n                alpha_star,\n                mu_star,\n                sigma_star,\n                gamma_length,\n                n_samples,\n            )\n        ).square()\n    if get_time_resolved:\n        return scores, scores.mean()\n    else:\n        return scores.mean()\n\n\n@ct.electron\ndef get_norm_and_anom_scores(\n    X_norm: torch.Tensor,\n    X_anom: torch.Tensor,\n    T: torch.Tensor,\n    callable_proj: callable,\n    model_params: dict,\n    gamma_length: int,\n    n_samples: int,\n) -> torch.Tensor:\n    \"\"\"Get the anomaly scores assigned to input normal and anomalous time series instances.\n    model_params is a dictionary containing the optimal model parameters.\n    \"\"\"\n    alpha = model_params[\"alpha\"]\n    mu = model_params[\"mu\"]\n    sigma = model_params[\"sigma\"]\n    norm_scores = torch.tensor(\n        [\n            get_anomaly_score(callable_proj, xt, T, alpha, mu, sigma, gamma_length, n_samples)\n            for xt in X_norm\n        ]\n    )\n    anom_scores = torch.tensor(\n        [\n            get_anomaly_score(callable_proj, xt, T, alpha, mu, sigma, gamma_length, n_samples)\n            for xt in X_anom\n        ]\n    )\n    return torch.cat([norm_scores, anom_scores])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now create our second `@ct.lattice`. We are to test the optimal model\nagainst two random models. If our model is trainable, we should see that\nthe trained model is better.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@ct.lattice\ndef threshold_tuning_workflow(\n    opt_params: dict,\n    gamma_length: int,\n    n_samples: int,\n    probs_func: callable,\n    zeta_min: float,\n    zeta_max: float,\n    steps: int,\n    p: int,\n    num_series: int,\n    noise_amp: float,\n    spike_amp: float,\n    max_duration: int,\n    t_init: float,\n    t_end: float,\n    k: int,\n    U: callable,\n    W: callable,\n    D: callable,\n    n_qubits: int,\n    random_model_seeds: torch.Tensor,\n    W_layers: int,\n) -> tuple:\n    \"\"\"A workflow for tuning the threshold value zeta, in order to maximize the accuracy score\n    for a validation data set. Results are tested against random models at their optimal zetas.\n    \"\"\"\n    # Generate datasets\n    X_val_norm, T = generate_normal_time_series_set(p, num_series, noise_amp, t_init, t_end)\n    X_val_anom, T = generate_anomalous_time_series_set(\n        p, num_series, noise_amp, spike_amp, max_duration, t_init, t_end\n    )\n    truth_labels = get_truth_labels(X_val_norm, X_val_anom)\n\n    # Initialize quantum functions\n    callable_proj = get_callable_projector_func(k, U, W, D, n_qubits, probs_func)\n\n    accs_list = []\n    scores_list = []\n    # Evaluate optimal model\n    scores = get_norm_and_anom_scores(\n        X_val_norm, X_val_anom, T, callable_proj, opt_params, gamma_length, n_samples\n    )\n    accs_opt = threshold_scan_acc_score(scores, truth_labels, zeta_min, zeta_max, steps)\n    accs_list.append(accs_opt)\n    scores_list.append(scores)\n\n    # Evaluate random models\n    for seed in random_model_seeds:\n        rand_params = get_initial_parameters(W, W_layers, n_qubits, seed)\n        scores = get_norm_and_anom_scores(\n            X_val_norm, X_val_anom, T, callable_proj, rand_params, gamma_length, n_samples\n        )\n        accs_list.append(threshold_scan_acc_score(scores, truth_labels, zeta_min, zeta_max, steps))\n        scores_list.append(scores)\n    return accs_list, scores_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now set the input parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "threshold_tuning_options = {\n    \"spike_amp\": 0.4,\n    \"max_duration\": 5,\n    \"zeta_min\": 0,\n    \"zeta_max\": 1,\n    \"steps\": 100000,\n    \"random_model_seeds\": [0, 1],\n    \"W_layers\": 2,\n    \"opt_params\": results_dict[\"opt_params\"],\n}\n\nthreshold_tuning_options.update(general_options)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As before, we dispatch the lattice to the `Covalent` server.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "val_dispatch_id = ct.dispatch(threshold_tuning_workflow)(**threshold_tuning_options)\nct_val_results = ct.get_result(dispatch_id=val_dispatch_id, wait=True)\naccs_list, scores_list = ct_val_results.result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we can plot the results:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "zeta_xlims = [(0, 0.001), (0.25, 0.38), (0.25, 0.38)]\ntitles = [\"Trained model\", \"Random model 1\", \"Random model 2\"]\nzetas = torch.linspace(\n    threshold_tuning_options[\"zeta_min\"],\n    threshold_tuning_options[\"zeta_max\"],\n    threshold_tuning_options[\"steps\"],\n)\nfig, axs = plt.subplots(ncols=3, nrows=2, sharey=\"row\")\nfor i in range(3):\n    axs[0, i].plot(zetas, accs_list[i])\n    axs[0, i].set_xlim(zeta_xlims[i])\n    axs[0, i].set_xlabel(\"Threshold [$\\zeta$]\")\n    axs[0, i].set_title(titles[i])\n    axs[1, i].boxplot(\n        [\n            scores_list[i][0 : general_options[\"num_series\"]],\n            scores_list[i][general_options[\"num_series\"] : -1],\n        ],\n        labels=[\"Normal\", \"Anomalous\"],\n    )\n    axs[1, i].set_yscale(\"log\")\n    axs[1, i].axhline(\n        zetas[torch.argmax(accs_list[i])], color=\"k\", linestyle=\":\", label=\"Optimal $\\zeta$\"\n    )\n    axs[1, i].legend()\naxs[0, 0].set_ylabel(\"Accuracy score\")\naxs[1, 0].set_ylabel(\"Anomaly score [$a_X(y)$]\")\nfig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Parsing the above, we can see that the optimal model achieves high\naccuracy when the threshold is tuned using the validation data. On the\nother hand, the random models return mostly random results (sometimes\neven worse than random guesses), regardless of how we set the threshold.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Testing the model\n=================\n\nNow with optimal thresholds for our optimized and random models, we can\nperform testing. We already have all of the electrons to do this, so we\ncreate the `@ct.lattice`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@ct.lattice\ndef testing_workflow(\n    opt_params: dict,\n    gamma_length: int,\n    n_samples: int,\n    probs_func: callable,\n    best_zetas: list,\n    p: int,\n    num_series: int,\n    noise_amp: float,\n    spike_amp: float,\n    max_duration: int,\n    t_init: float,\n    t_end: float,\n    k: int,\n    U: callable,\n    W: callable,\n    D: callable,\n    n_qubits: int,\n    random_model_seeds: torch.Tensor,\n    W_layers: int,\n) -> list:\n    \"\"\"A workflow for calculating anomaly scores for a set of testing time series\n    given an optimal model and set of random models. We use the optimal zetas found in threshold tuning.\n    \"\"\"\n    # Generate time series\n    X_val_norm, T = generate_normal_time_series_set(p, num_series, noise_amp, t_init, t_end)\n    X_val_anom, T = generate_anomalous_time_series_set(\n        p, num_series, noise_amp, spike_amp, max_duration, t_init, t_end\n    )\n    truth_labels = get_truth_labels(X_val_norm, X_val_anom)\n\n    # Prepare quantum functions\n    callable_proj = get_callable_projector_func(k, U, W, D, n_qubits, probs_func)\n\n    accs_list = []\n    # Evaluate optimal model\n    scores = get_norm_and_anom_scores(\n        X_val_norm, X_val_anom, T, callable_proj, opt_params, gamma_length, n_samples\n    )\n    preds = get_preds_given_threshold(best_zetas[0], scores)\n    accs_list.append(get_accuracy_score(preds, truth_labels))\n    # Evaluate random models\n    for zeta, seed in zip(best_zetas[1:], random_model_seeds):\n        rand_params = get_initial_parameters(W, W_layers, n_qubits, seed)\n        scores = get_norm_and_anom_scores(\n            X_val_norm, X_val_anom, T, callable_proj, rand_params, gamma_length, n_samples\n        )\n        preds = get_preds_given_threshold(zeta, scores)\n        accs_list.append(get_accuracy_score(preds, truth_labels))\n    return accs_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We dispatch it to the Covalent server with the appropriate parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "testing_options = {\n    \"spike_amp\": 0.4,\n    \"max_duration\": 5,\n    \"best_zetas\": [zetas[torch.argmax(accs)] for accs in accs_list],\n    \"random_model_seeds\": [0, 1],\n    \"W_layers\": 2,\n    \"opt_params\": results_dict[\"opt_params\"],\n}\n\ntesting_options.update(general_options)\n\ntest_dispatch_id = ct.dispatch(testing_workflow)(**testing_options)\nct_test_results = ct.get_result(dispatch_id=test_dispatch_id, wait=True)\naccs_list = ct_test_results.result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we plot the results below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure()\nplt.bar([1, 2, 3], accs_list)\nplt.axhline(0.5, color=\"k\", linestyle=\":\", label=\"Random accuracy\")\nplt.xticks([1, 2, 3], [\"Trained model\", \"Random model 1\", \"Random model 2\"])\nplt.ylabel(\"Accuracy score\")\nplt.title(\"Accuracy scores for trained and random models\")\nleg = plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As can be seen, once more, the trained model is far more accurate than\nthe random models. Awesome! Now that we\\'re done with the calculations\nin this tutorial, we just need to remember to shut down the Covalent\nserver\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Shut down the covalent server\nstop = os.system(\"covalent stop\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Conclusions\n===========\n\nWe\\'ve now reached the end of this tutorial! Quickly recounting what we\nhave learnt, we:\n\n1.  Learnt the background of how to detect anomalous time series\n    instances, *quantumly*,\n2.  Learnt how to build the code to achieve this using PennyLane and\n    PyTorch, and,\n3.  Learnt the basics of Covalent: a workflow orchestration tool for\n    heterogeneous computation\n\nIf you want to learn more about QVR, you should consult the paper where\nwe generalize the math a little and test the algorithm on less trivial\ntime series data than was dealt with in this tutorial. We also ran some\nexperiments on real quantum computers, enhancing our results using error\nmitigation techniques. If you want to play some more with Covalent,\ncheck us out on [GitHub](https://github.com/AgnostiqHQ/covalent/) and/or\nengage with other tutorials in our\n[documentation](https://covalent.readthedocs.io/en/stable/).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "References\n==========\n\nAbout the authors \\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\--.. include::\n../\\_static/authors/jack\\_stephen\\_baker.txt .. include::\n../\\_static/authors/santosh\\_kumar\\_radha.txt\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}